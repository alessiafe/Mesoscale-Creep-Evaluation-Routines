{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store mechanosorptive results into Excel files\n",
    "This notebook store the results of mechanosorptive experiments into Excel files organized by sample type. Each file contains multiple sheets (one per experiment), and each sheet reports detailed sample information (dimensions, applied load, loading degree) along with time, creep strain, creep compliance, and moisture measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import libraries and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from mat73 import loadmat\n",
    "from math import *\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import lsq_linear\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import trim_mean\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "folder_path = '/home/aferrara/Desktop/Creep_experiments_1/'\n",
    "savepath = '/home/aferrara/Desktop/creep-evaluation-routine/ms_creep_python_routine/MS_Creep_Tests_Dataset'\n",
    "# Set file paths\n",
    "dvs_path = savepath + '/dvs_data.npz'\n",
    "elastic_path = savepath + 'elastic_compliances_Ferrara_Wittel_2024.csv'\n",
    "viscoel_path = savepath + 'master_vec_prony_param.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Customized functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve experiment path\n",
    "def get_rawdata_path(exp_code, main_path):\n",
    "    # Construct experiment path\n",
    "    exp_path = os.path.join(main_path, exp_code)\n",
    "    # Check if folder exists\n",
    "    if not os.path.isdir(exp_path): raise FileNotFoundError(f\"There is no folder for the experiment ({exp_path}).\")\n",
    "    # Check if the folder is empty\n",
    "    if not os.listdir(exp_path): raise FileNotFoundError(f\"The folder for the experiment exists but is empty ({exp_path}).\")\n",
    "    return exp_path\n",
    "# end: get_rawdata_path\n",
    "\n",
    "# Retrieve experiment data\n",
    "def get_experiment_data(exp_code, main_path):\n",
    "\n",
    "    # Get experiment folder path\n",
    "    exp_path = get_rawdata_path(exp_code, main_path)\n",
    "    data_path = os.path.join(exp_path, f\"{exp_code}_Data_new.txt\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(data_path): raise FileNotFoundError(f\"Experiment data file '{data_path}' not found.\")\n",
    "\n",
    "    # Read the data file into a Pandas DataFrame\n",
    "    df = pd.read_csv(data_path, delimiter='\\t')  # adjust delimiter if needed\n",
    "\n",
    "    # Convert DataFrame rows to a list of dictionaries\n",
    "    samples_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        sample = {\n",
    "            \"sample_holder\": row[0],    # Column 1\n",
    "            \"exp_name\": row[2],         # Column 3\n",
    "            \"img_name\": row[3],         # Column 4\n",
    "            \"thick\": row[10],           # Column 11\n",
    "            \"width\": row[11],           # Column 12\n",
    "            \"area\": row[10] * row[11],  # Column 11 * Column 12\n",
    "            \"load\": row[6],             # Column 7\n",
    "            \"failure_load\": row[7],     # Column 8\n",
    "            \"initial_load\": row[8],     # Column 9\n",
    "            \"load_cell\": row[9],        # Column 10\n",
    "            \"RH\": row[12]               # Column 13\n",
    "        }\n",
    "        samples_data.append(sample)\n",
    "\n",
    "    return samples_data, exp_path\n",
    "# end: def get_experiment_data\n",
    "\n",
    "# Retrieve all unique sample types across experiments\n",
    "def get_sample_types(main_path, prefix=\"MST30-90\"):\n",
    "    folders = sorted(\n",
    "        [d for d in os.listdir(main_path)\n",
    "         if os.path.isdir(os.path.join(main_path, d)) and d.startswith(prefix)])\n",
    "    types = set()\n",
    "    for exp in folders:\n",
    "        samples_data, _ = get_experiment_data(exp, main_path)\n",
    "        for sample in samples_data:\n",
    "            st = sample['exp_name'][3:5]\n",
    "            if st == 'LT':\n",
    "                tissue = \"EW\" if int(sample['exp_name'][-1]) >= 4 else \"LW\"\n",
    "                st = f\"LT-{tissue}\"\n",
    "            types.add(st)\n",
    "    return sorted(types)\n",
    "# end: def get_sample_types\n",
    "\n",
    "# Load strain results of DIC analysis\n",
    "def load_strain_struct(grey_folder):\n",
    "    # Find the first matching .mat file\n",
    "    export_file_strain = glob.glob(os.path.join(grey_folder, '*_strain_DIC.mat'))[0]\n",
    "    if not os.path.isfile(export_file_strain):\n",
    "        raise FileNotFoundError(f\"The data struct {os.path.basename(export_file_strain)} does not exist in {grey_folder}.\")\n",
    "    # Load mat file\n",
    "    export_strain = loadmat(export_file_strain)['new_struct']\n",
    "    # Convert MATLAB serial datenum to Python datetime \n",
    "    time_name = ['imgtime', 'loadtime', 'RHtime']\n",
    "    for name in time_name:\n",
    "        matlab_datenum = export_strain[name]  # MATLAB serial number\n",
    "        if isinstance(matlab_datenum, np.ndarray):  # If it's an array, process each element\n",
    "            export_strain[name] = np.array([\n",
    "                (datetime.fromordinal(int(d)) + timedelta(days=d % 1) - timedelta(days=366)).replace(microsecond=0) for d in matlab_datenum])\n",
    "        else:  # If it's a single value, process directly\n",
    "            export_strain[name] = (datetime.fromordinal(int(matlab_datenum)) + timedelta(days=matlab_datenum % 1) - timedelta(days=366)).replace(microsecond=0)\n",
    "\n",
    "    return export_strain\n",
    "# end: def load_strain_struct\n",
    "\n",
    "# Locate previous and next index of drop \n",
    "def locate_drop_idx(cycle_idx, drop_idx, threshold=3):\n",
    "\n",
    "    # Compute distances to each cycle boundary\n",
    "    distances = np.abs(cycle_idx - drop_idx)\n",
    "    min_dist = distances.min()\n",
    "\n",
    "    if min_dist <= threshold:\n",
    "        # If drop too close, snap to the nearest cycle boundary\n",
    "        nearest = cycle_idx[np.argmin(distances)]\n",
    "        drop_idx = nearest\n",
    "        # Only look for the cycle boundary before drop\n",
    "        before = cycle_idx[cycle_idx < drop_idx]\n",
    "        before_idx = before.max() if before.size else None\n",
    "        after_idx = drop_idx\n",
    "    else:\n",
    "        # If drop is not too close, find both before and after\n",
    "        before = cycle_idx[cycle_idx < drop_idx]\n",
    "        after  = cycle_idx[cycle_idx > drop_idx]\n",
    "        before_idx = before.max() if before.size else None\n",
    "        after_idx  = after.min() if after.size  else None\n",
    "\n",
    "    return before_idx, drop_idx, after_idx\n",
    "# end: locate_drop_idx\n",
    "\n",
    "# Collect all samples matching a given sample type\n",
    "def get_samples_by_type(sample_type, main_path, prefix=\"MST30-90\"):\n",
    "    \n",
    "    matches = []\n",
    "    folders = sorted(\n",
    "        [d for d in os.listdir(main_path)\n",
    "         if os.path.isdir(os.path.join(main_path, d)) and d.startswith(prefix)])\n",
    "\n",
    "    for exp in folders:\n",
    "        samples_data, exp_path = get_experiment_data(exp, main_path)\n",
    "        for s in samples_data[::2]:\n",
    "            st = s['exp_name'][3:5]\n",
    "            if st == 'LT':\n",
    "                tissue = \"EW\" if int(s['exp_name'][-1]) >= 4 else \"LW\"\n",
    "                st = f\"LT-{tissue}\"\n",
    "            if st == sample_type:\n",
    "                matches.append((exp, exp_path, f\"{s['exp_name']}_{s['sample_holder']}\"))\n",
    "                \n",
    "    return matches\n",
    "# end: get_samples_by_type\n",
    "\n",
    "# Estract loading degree of a given sample\n",
    "def extract_loading_deg(exp_path, sample_name):\n",
    "\n",
    "    # Set path to image folder\n",
    "    file_folder = os.path.join(exp_path, \"Results\", f\"{sample_name}\")\n",
    "    # Check if folder exists\n",
    "    if not os.path.isdir(file_folder): raise FileNotFoundError(f\"The folder {file_folder} does not exist.\")\n",
    "    # Load data\n",
    "    strain_results = load_strain_struct(file_folder)\n",
    "\n",
    "    return strain_results['loading_deg']\n",
    "# end: def extract_loading_deg\n",
    "\n",
    "# Find indexes of local stress drop\n",
    "def find_local_drop(arr, stress_drop=None, window=5):\n",
    "\n",
    "    # Compute all differences\n",
    "    arr = np.asarray(arr)\n",
    "    n = arr.size\n",
    "    if n < 2: raise ValueError(\"Array must have at least 2 elements to find a drop.\")\n",
    "    diffs = np.diff(arr)\n",
    "    \n",
    "    if stress_drop is None:\n",
    "        # If no guess, then global search\n",
    "        return int(np.argmin(diffs)) + 1\n",
    "\n",
    "    # Clamp stress_drop to valid [0, n-1]\n",
    "    sd = int(np.clip(stress_drop, 0, n - 1))\n",
    "    # Convert to diffs‐index domain: diffs[j] = arr[j+1] - arr[j]\n",
    "    # so arr‐index i corresponds to diffs‐index j = i-1\n",
    "    j_center = sd - 1\n",
    "\n",
    "    # Define window in diffs‐space\n",
    "    j0 = max(0, j_center - window)\n",
    "    j1 = min(n - 2, j_center + window)\n",
    "\n",
    "    # Find drop\n",
    "    local = diffs[j0:j1+1]\n",
    "    if local.size:\n",
    "        j_drop = j0 + int(np.argmin(local))\n",
    "        return j_drop + 1\n",
    "\n",
    "    # If for some reason the window was empty,\n",
    "    # fall back to global search\n",
    "    return int(np.argmin(diffs)) + 1\n",
    "# end: def find_local_drop\n",
    "\n",
    "# Build identical moisture cycles with corresponding time and stress\n",
    "def build_cycles(tdata, wdata, total_cycles, stress_value, stressed_cycles):\n",
    "\n",
    "    # Convert to arrays\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    if tdata.shape != wdata.shape: raise ValueError(\"tdata and wdata must have the same shape\")\n",
    "\n",
    "    # Determine cycle period\n",
    "    period = tdata[-1] - tdata[0] + tdata[-1] - tdata[-2]\n",
    "\n",
    "    # Pre-allocate arrays\n",
    "    n_per_cycle = tdata.size\n",
    "    N = n_per_cycle * total_cycles\n",
    "    t_full = np.empty(N, dtype=tdata.dtype)\n",
    "    w_full = np.empty(N, dtype=wdata.dtype)\n",
    "    s_full = np.empty(N, dtype=float)\n",
    "\n",
    "    for cycle in range(total_cycles):\n",
    "        start = cycle * n_per_cycle\n",
    "        end = start + n_per_cycle\n",
    "\n",
    "        # Time offset\n",
    "        t_full[start:end] = tdata + cycle * period\n",
    "        w_full[start:end] = wdata\n",
    "\n",
    "        # Apply stress for initial cycles, else 0\n",
    "        if cycle < stressed_cycles:\n",
    "            s_full[start:end] = stress_value\n",
    "        elif cycle == stressed_cycles:\n",
    "            s_full[start-1] = 0.0\n",
    "            s_full[start:end] = 0.0\n",
    "        else:\n",
    "            s_full[start:end] = 0.0\n",
    "\n",
    "    return t_full, w_full, s_full\n",
    "# end: build_cycles\n",
    "\n",
    "# Build moisture cycles with different first sorption with corresponding time and stress\n",
    "def build_cycles_with_initial(tdata0, wdata0, tdata, wdata, total_cycles, stress_value, stressed_cycles):\n",
    "    \n",
    "    # Convert to arrays\n",
    "    t0 = np.asarray(tdata0)\n",
    "    w0 = np.asarray(wdata0)\n",
    "    t  = np.asarray(tdata)\n",
    "    w  = np.asarray(wdata)\n",
    "\n",
    "    # Determine cycle period\n",
    "    dt = t[-1] - t[-2]\n",
    "    period = (t[-1] - t[0]) + dt\n",
    "\n",
    "    # Total length\n",
    "    n0 = t0.size\n",
    "    n  = t.size\n",
    "    N  = n0 + n*(total_cycles-1)\n",
    "    # Pre-allocate\n",
    "    t_full = np.empty(N, dtype=t.dtype)\n",
    "    w_full = np.empty(N, dtype=w.dtype)\n",
    "    s_full = np.empty(N, dtype=float)\n",
    "\n",
    "    # Set first cycle\n",
    "    t_full[:n0] = t0\n",
    "    w_full[:n0] = w0\n",
    "    s_full[:n0] = stress_value if 0 < stressed_cycles else 0.0\n",
    "    # Set remaining cycles\n",
    "    for i in range(1, total_cycles):\n",
    "        start = n0 + (i-1)*n\n",
    "        end   = start + n\n",
    "        # Time shift\n",
    "        t_full[start:end] = t + (i-1)*period\n",
    "        w_full[start:end] = w\n",
    "        # Apply stress for initial cycles, else 0\n",
    "        if i < stressed_cycles:\n",
    "            s_full[start:end] = stress_value\n",
    "        elif i == stressed_cycles:\n",
    "            s_full[start-1] = 0.0\n",
    "            s_full[start:end] = 0.0\n",
    "        else:\n",
    "            s_full[start:end] = 0.0\n",
    "\n",
    "    return t_full, w_full, s_full\n",
    "# end: build_cycles_with_initial\n",
    "\n",
    "# Build moisture cycles with different first sorption from dvs data with corresponding time and stress\n",
    "def build_dynamic_cycles(tdata0, wdata0, tdata, wdata, t_exp, cycle_idx, idx_before, idx_drop, stress_val):\n",
    "    \n",
    "    # Inititalize arrays\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    if tdata.shape != wdata.shape: raise ValueError(\"tdata and wdata must have the same shape\")\n",
    "    \n",
    "    # Create loading cycles with first sorption different\n",
    "    n_pre = np.searchsorted(cycle_idx, idx_before,  side=\"right\") + 1 if len(stress_val) > 1 else np.searchsorted(cycle_idx, idx_before,  side=\"right\")\n",
    "    t_full, w_full, s_full = build_cycles_with_initial(tdata0, wdata0, tdata, wdata, n_pre, stress_val[0], n_pre)\n",
    "\n",
    "    if t_full[-1] > t_exp[idx_drop]:\n",
    "        idx_closest = np.abs(t_full - t_exp[idx_drop-2]).argmin()\n",
    "        s_full = s_full[:idx_closest+1]\n",
    "        t_full = t_full[:idx_closest+1]\n",
    "        w_full = w_full[:idx_closest+1]\n",
    "\n",
    "    return t_full, w_full, s_full\n",
    "# end: build_dynamic_cycles\n",
    "\n",
    "# Find end index of each moisture cycle\n",
    "def find_cycle_ends(w, tol=1e-5, merge_gap=5):\n",
    "    w = np.asarray(w)\n",
    "    w_min = w.min()\n",
    "\n",
    "    # Find all 'low plateau' indices\n",
    "    low_idxs = np.where(w <= (w_min + tol))[0]\n",
    "    if low_idxs.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Cluster them by index proximity\n",
    "    clusters = [[low_idxs[0]]]\n",
    "    for idx in low_idxs[1:]:\n",
    "        if idx - clusters[-1][-1] <= merge_gap:\n",
    "            # Same plateau cluster\n",
    "            clusters[-1].append(idx)\n",
    "        else:\n",
    "            # New plateau cluster\n",
    "            clusters.append([idx])\n",
    "\n",
    "    # Pick the largest index from each cluster\n",
    "    cycle_end_indices = [max(cluster) for cluster in clusters]\n",
    "\n",
    "    return cycle_end_indices\n",
    "# end: def find_cycle_ends\n",
    "\n",
    "# Convert RH into mean w (mean S/D of tissues from standard dvs test)\n",
    "def RH_to_w_mean(x):\n",
    "    w = 7.155e-11 * x**5 - 1.659e-08 * x**4 + 1.75e-06 * x**3 - 9.343e-05 * x**2 + 0.003795 * x + 0.002295\n",
    "    return w\n",
    "# end: RH_to_w_mean\n",
    "\n",
    "# Calculate hygroexpansion strain\n",
    "def calculate_hygroexp_strain(sample_type, wdata):\n",
    "\n",
    "    # Set hygroexpansion coefficients\n",
    "    alpha = {\"R\": 0.182,\n",
    "            \"T\": 0.343,\n",
    "            \"L\": 0.0061}\n",
    "    # Extract loading direction\n",
    "    long_dir = sample_type[0]\n",
    "    # Calculate hygroexpansion strain\n",
    "    eps_w = alpha[long_dir]*(wdata-wdata[0])\n",
    "\n",
    "    return eps_w\n",
    "# end: def calculate_hygroexp_strain\n",
    "\n",
    "# Fit elastic compliances to moisture\n",
    "def fit_elastic_comp(elastic_path):\n",
    "\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(elastic_path)\n",
    "    # Group and compute mean compliance\n",
    "    mean_df = (df.groupby(['sample_type', 'RH'], as_index=False).C0.mean().rename(columns={'C0': 'C0_mean'}))\n",
    "    # Calculate average w for each RH\n",
    "    mean_df['w'] = RH_to_w_mean(mean_df['RH'])\n",
    "    # Fit quadratic function in w\n",
    "    el_fits = {}\n",
    "    for stype, sub in mean_df.groupby('sample_type'):\n",
    "        x = sub['w'].values\n",
    "        y = sub['C0_mean'].values\n",
    "        A = np.vstack(( x**2, x, np.ones_like(x) )).T\n",
    "        # Choose bounds per sample_type\n",
    "        if stype in (\"LR\", \"LT-LW\"):\n",
    "            lower = [-np.inf, 0.0, 0.0]\n",
    "            upper = [0.0, np.inf, np.inf]\n",
    "        else:\n",
    "            lower = [0.0, 0.0, 0.0]\n",
    "            upper = [np.inf, np.inf, np.inf]\n",
    "        # Solve\n",
    "        res = lsq_linear(A, y, bounds=(lower, upper))\n",
    "        a, b, c = res.x\n",
    "        el_fits[stype] = (a, b, c)\n",
    "\n",
    "    return el_fits\n",
    "# end: fit_elastic_comp\n",
    "\n",
    "# Calculate elastic strain\n",
    "def calculate_elastic_strain(comp_el, sample_type, stress, wdata, load=True):\n",
    "\n",
    "    # Initialize array\n",
    "    N = len(wdata)\n",
    "    eps_el = [0.0] * N\n",
    "\n",
    "    if not load:\n",
    "        # All zeros if not loading\n",
    "        return eps_el\n",
    "\n",
    "    # Extract the three coefficients for compliance: a, b, c\n",
    "    a, b, c = comp_el[sample_type]\n",
    "\n",
    "    for i in range(N):\n",
    "        # Compute compliance at wdata[i]\n",
    "        C_i = a * wdata[i]**2 + b * wdata[i] + c\n",
    "        # Instantaneous elastic strain = compliance * stress\n",
    "        eps_el[i] = C_i * stress[i]\n",
    "\n",
    "    return eps_el\n",
    "# end: def calculate_elastic_strain\n",
    "\n",
    "# Pick local strain minimum closest to end cycle\n",
    "def snap_minima_to_cycles(min_indices, new_cycle_idx, *, max_gap=None, unique=True):\n",
    "    \n",
    "    min_indices = np.asarray(min_indices)\n",
    "    anchors = np.asarray(new_cycle_idx)\n",
    "\n",
    "    if min_indices.size == 0 or anchors.size == 0:\n",
    "        return np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int)\n",
    "\n",
    "    # Work on a sorted copy of minima for fast nearest-neighbor via searchsorted\n",
    "    mins_sorted = np.sort(np.unique(min_indices))\n",
    "    pos = np.searchsorted(mins_sorted, anchors)\n",
    "\n",
    "    left_idx = np.clip(pos - 1, 0, len(mins_sorted) - 1)\n",
    "    right_idx = np.clip(pos,       0, len(mins_sorted) - 1)\n",
    "\n",
    "    left_vals = mins_sorted[left_idx]\n",
    "    right_vals = mins_sorted[right_idx]\n",
    "\n",
    "    # Choose the closer side\n",
    "    choose_right = np.abs(right_vals - anchors) < np.abs(left_vals - anchors)\n",
    "    nearest = np.where(choose_right, right_vals, left_vals)\n",
    "    dist = np.abs(nearest - anchors)\n",
    "\n",
    "    # Apply max_gap filter if requested\n",
    "    keep = np.ones(len(anchors), dtype=bool)\n",
    "    if max_gap is not None:\n",
    "        keep &= (dist <= max_gap)\n",
    "\n",
    "    nearest = nearest[keep]\n",
    "    anchors_kept = anchors[keep]\n",
    "    dist = dist[keep]\n",
    "\n",
    "    if not unique or nearest.size == 0:\n",
    "        return nearest, anchors_kept, dist\n",
    "\n",
    "    # Resolve duplicates: keep the anchor with smallest distance per chosen minimum\n",
    "    # (ties resolved by first occurrence)\n",
    "    order = np.lexsort((np.arange(len(nearest)), dist))  # sort by distance, then stable index\n",
    "    nearest_sorted = nearest[order]\n",
    "    anchors_sorted = anchors_kept[order]\n",
    "    dist_sorted = dist[order]\n",
    "\n",
    "    # Keep first occurrence of each chosen minimum\n",
    "    uniq_mask = np.ones_like(nearest_sorted, dtype=bool)\n",
    "    uniq_mask[1:] = nearest_sorted[1:] != nearest_sorted[:-1]\n",
    "\n",
    "    picked_minima = nearest_sorted[uniq_mask]\n",
    "    picked_anchors = anchors_sorted[uniq_mask]\n",
    "    picked_dist = dist_sorted[uniq_mask]\n",
    "\n",
    "    # (Optional) restore original anchor order\n",
    "    sort_back = np.argsort(np.argsort(picked_anchors))\n",
    "    return picked_minima[sort_back], picked_anchors[sort_back], picked_dist[sort_back]\n",
    "# end: def snap_minima_to_cycles\n",
    "\n",
    "# Fit viscoelastic compliances (prony coeff.) to moisture\n",
    "def fit_viscoelastic_comp(viscoel_path):\n",
    "\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(viscoel_path, sep=\",\", dtype={\"sample_type\": str, \"RH\": float, \"avg_comp_i\": str})\n",
    "    # Parse string‐list into float-list\n",
    "    df[\"comp_i\"] = df[\"avg_comp_i\"].apply(lambda s: [float(x) for x in ast.literal_eval(s)])\n",
    "    # Calculate average w for each RH\n",
    "    df[\"w\"] = RH_to_w_mean(df[\"RH\"]) \n",
    "    # Fit quadratic function in w\n",
    "    ve_fits = {}\n",
    "    for stype, sub in df.groupby(\"sample_type\"):\n",
    "        w_vals = sub[\"w\"].values\n",
    "        C = np.vstack(sub[\"comp_i\"].values)\n",
    "        polys = [np.polyfit(w_vals, C[:, j], 2) for j in range(C.shape[1])]\n",
    "        ve_fits[stype] = polys\n",
    "\n",
    "    return ve_fits\n",
    "# end: def fit_viscoelastic_comp\n",
    "\n",
    "# Calculate prony series\n",
    "def prony_response(comp_i, tdata, tau_0):\n",
    "    return (np.sum(comp_i) - np.sum(comp_i[:, None] * np.exp(-tdata / tau_0[:, None]), axis=0))\n",
    "# end: def prony_response\n",
    "\n",
    "# Calculate prony coefficients at w\n",
    "def comp_i_at_w(comp_ve, stype, wval):\n",
    "    polys = comp_ve[stype]\n",
    "    return np.array([ np.polyval(p, wval) for p in polys ])\n",
    "# end: def comp_i_at_w\n",
    "\n",
    "# Calculate viscoelastic creep compliance at given time and w\n",
    "def compliance_at_t_w(comp_ve, stype, tdata, wval, tau_0):\n",
    "    c_i = comp_i_at_w(comp_ve, stype, wval)\n",
    "    return prony_response(c_i, tdata, tau_0)\n",
    "# end: def compliance_at_t_w\n",
    "\n",
    "# Calculate viscoelastic strain\n",
    "def calculate_viscoelastic_strain(comp_ve, sample_type, stress, tdata, wdata, eps_el0):\n",
    "\n",
    "    # Set retardation times [h]\n",
    "    tau_0 = np.array([0.1, 1., 10., 100.])\n",
    "    # Calculate viscoelastic strain\n",
    "    eps_ve = []\n",
    "    eps_i = np.zeros(tau_0.shape)\n",
    "    eps_ve.append(eps_el0)\n",
    "    for i in range(1, len(tdata)):\n",
    "        # Calculate viscoelastic strain components\n",
    "        comp_i_n1 = comp_i_at_w(comp_ve, sample_type, wdata[i])\n",
    "        comp_i_n0 = comp_i_at_w(comp_ve, sample_type, wdata[i-1])\n",
    "        comp_i = (comp_i_n1 + comp_i_n0) /2.\n",
    "        deps_ve = np.zeros(tau_0.shape)\n",
    "        deps_ve = 1. / tau_0 * (comp_i * stress[i] - eps_i)\n",
    "        eps_i += deps_ve * (tdata[i]-tdata[i-1])\n",
    "        eps_ve.append(np.sum(eps_i)+eps_el0)\n",
    "\n",
    "    return eps_ve\n",
    "# end: def calculate_viscoelastic_strain\n",
    "\n",
    "# Calculate reference creep curves\n",
    "def calculate_ref_viscoel_curves(comp_ve, sample_type, stress, tdata, wdata, eps_el0):\n",
    "\n",
    "    # Set retardation times [h]\n",
    "    tau_0 = np.array([0.1, 1., 10., 100.])\n",
    "    # Calculate ref. curve at 30% RH = 0.07 mc\n",
    "    eps_ve_ref30 = compliance_at_t_w(comp_ve, sample_type, tdata, min(wdata), tau_0) * stress + eps_el0\n",
    "    # Calculate ref. curve at 90% RH = 0.20 mc\n",
    "    eps_ve_ref90 = compliance_at_t_w(comp_ve, sample_type, tdata, max(wdata), tau_0) * stress + eps_el0\n",
    "\n",
    "    return eps_ve_ref30, eps_ve_ref90\n",
    "# end: def calculate_ref_viscoel_curves\n",
    "\n",
    "# Fit monotonic regression\n",
    "def unimodal_fit(x, y):\n",
    "\n",
    "    # Initializations       \n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    N = len(y)\n",
    "    best_err = np.inf\n",
    "    best_fit = None\n",
    "\n",
    "    # Try each possible peak-location k\n",
    "    for k in range(1, N-1):\n",
    "        # Rising isotonic on [0..k]\n",
    "        ir_inc = IsotonicRegression(increasing=True)\n",
    "        y_inc = ir_inc.fit_transform(x[:k+1], y[:k+1])\n",
    "\n",
    "        # Falling  isotonic on [k..N-1]\n",
    "        ir_dec = IsotonicRegression(increasing=False)\n",
    "        y_dec = ir_dec.fit_transform(x[k:], y[k:])\n",
    "\n",
    "        # Stitch (avoid doubling the k-th point)\n",
    "        y_fit = np.concatenate([y_inc[:-1], y_dec])\n",
    "        err = np.sum((y - y_fit)**2)\n",
    "        if err < best_err:\n",
    "            best_err  = err\n",
    "            best_fit  = y_fit\n",
    "\n",
    "    return best_fit\n",
    "# end: def unimodal_fit\n",
    "\n",
    "# Overlap and fit cycles\n",
    "def analyze_cycles(tdata, wdata, edata, w_cycle_idx, eps_cycle_idx, firstc=3, lastc=7, num_grid_points=1000):\n",
    "\n",
    "    # Convert to arrays\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    edata = np.asarray(edata)\n",
    "\n",
    "    # Compute relative indices and segment boundaries\n",
    "    firstc = firstc-2\n",
    "    if lastc!=-1:\n",
    "        lastc = lastc - 1\n",
    "        w_cycle_idx = w_cycle_idx[firstc:lastc+1]\n",
    "        eps_cycle_idx = eps_cycle_idx[firstc:lastc+1]\n",
    "    else:\n",
    "        w_cycle_idx = w_cycle_idx[firstc:]\n",
    "        eps_cycle_idx = eps_cycle_idx[firstc:]\n",
    "    w_segments = list(zip(w_cycle_idx, w_cycle_idx[1:]))\n",
    "    eps_segments = list(zip(eps_cycle_idx, eps_cycle_idx[1:]))\n",
    "\n",
    "    # Collect and shift cycles\n",
    "    cycles = []\n",
    "    for (w_start, w_end), (eps_start, eps_end) in zip(w_segments, eps_segments):\n",
    "        # W segment and its relative time (0..1)\n",
    "        t_w = tdata[w_start+1:w_end+1]\n",
    "        w_seg = wdata[w_start+1:w_end+1]\n",
    "        t_w_rel = (t_w - t_w[0]) / (t_w[-1] - t_w[0])\n",
    "        # E segment and its relative time (0..1)\n",
    "        t_e = tdata[eps_start+1:eps_end+1]\n",
    "        e_seg = edata[eps_start+1:eps_end+1]\n",
    "        t_e_rel = (t_e - t_e[0]) / (t_e[-1] - t_e[0])\n",
    "        # Interpolate strain on the w grid\n",
    "        e_on_w = np.interp(t_w_rel, t_e_rel, e_seg)\n",
    "        # Locate peak\n",
    "        peak_i = np.argmax(e_on_w)\n",
    "        peak_val = e_on_w[peak_i]\n",
    "        # Store cycle data\n",
    "        cycles.append({\n",
    "            't_seg_rel': t_w_rel,\n",
    "            'w':       w_seg,\n",
    "            'eps':       e_on_w,\n",
    "            'last_eps':  e_on_w[-1],\n",
    "            'peak_eps':  peak_val})\n",
    "\n",
    "    ######## BUILD TEMPLATE FOR ALL CYCLES (except 1) ########\n",
    "    # Use first cycle's peak as reference\n",
    "    ref_peak = cycles[0]['peak_eps']\n",
    "    # Apply vertical shift based on peaks\n",
    "    for c in cycles: c['eps_shifted'] = c['eps'] - c['peak_eps'] + ref_peak\n",
    "    # Build common time grid where all shifted cycles overlap\n",
    "    t_fit = np.linspace(0, 1, num_grid_points)\n",
    "\n",
    "    # Interpolate each shifted cycle onto the common grid\n",
    "    s_interp = np.vstack([c['eps_shifted'] for c in cycles])\n",
    "    avg_s_time = np.mean(s_interp, axis=0)\n",
    "    avg_s_time = avg_s_time - avg_s_time[0]\n",
    "    s_interp = np.vstack([UnivariateSpline(c['t_seg_rel'], c['eps_shifted'], k=5, s=0.00001)(t_fit) for c in cycles])\n",
    "    # Calculate average strain\n",
    "    avg_s_time = np.mean(s_interp, axis=0)\n",
    "    # Shift template to start from second cycle\n",
    "    shift = avg_s_time[0] - edata[eps_cycle_idx[0]+1]\n",
    "    avg_s_time = avg_s_time - shift\n",
    "\n",
    "    # Try to fit avoiding any possible final increase\n",
    "    if lastc != -1:       \n",
    "        # Fit template\n",
    "        y_fit = unimodal_fit(t_fit, avg_s_time)\n",
    "        x = t_fit\n",
    "        peak_idx = np.argmax(y_fit)\n",
    "        # Fit a strictly‐decreasing isotonic to the tail\n",
    "        ir_down = IsotonicRegression(increasing=False)\n",
    "        x_tail = x[peak_idx:]\n",
    "        y_tail = y_fit[peak_idx:]\n",
    "        y_tail_iso = ir_down.fit_transform(x_tail, y_tail)\n",
    "        # Stitch it back together\n",
    "        avg_s_time = np.concatenate([y_fit[:peak_idx], y_tail_iso])\n",
    "\n",
    "\n",
    "    ######## BUILD TEMPLATE FOR 1st CYCLE ########\n",
    "    # Compute delta sorption\n",
    "    w0 = wdata[:w_cycle_idx[0]+1]\n",
    "    w1 = wdata[w_cycle_idx[0]+1:w_cycle_idx[1]+1]\n",
    "    idx1 = w1.argmax()\n",
    "    shift = w1[idx1] - w0[idx1]\n",
    "    w1 = w1 - shift\n",
    "    delta_w = [x/y for x, y in zip(w0, w1)]\n",
    "\n",
    "    # Pick target times\n",
    "    t_rel = (tdata[w_cycle_idx[0]+1:w_cycle_idx[1]+1] - tdata[w_cycle_idx[0]+1]) / (tdata[w_cycle_idx[1]+1] - tdata[w_cycle_idx[0]+1])\n",
    "    t0_rel = t_rel[idx1]\n",
    "    idx_fit = np.searchsorted(t_fit, t0_rel)\n",
    "    slice_times = t_fit[:idx_fit+1]\n",
    "    # Calculate scaling factor by interpolating delta_w\n",
    "    scale = np.interp(slice_times, t_rel[:idx1+1], delta_w[:idx1+1])\n",
    "    # Scale strain  proportional to sorption\n",
    "    avg_s_time0 = np.concatenate((avg_s_time[:idx_fit+1] * scale, avg_s_time[idx_fit+1:]), axis=0)\n",
    "    # Shift template I to start from elastic strain\n",
    "    shift = avg_s_time0[0] - edata[0]\n",
    "    avg_s_time0 = avg_s_time0 - shift\n",
    "\n",
    "    # Shift template II to start from 0\n",
    "    shift = avg_s_time[0]\n",
    "    avg_s_time = avg_s_time - shift\n",
    "    \n",
    "    return [t_fit, avg_s_time0, avg_s_time]\n",
    "# end: def analyze_cycles\n",
    "\n",
    "# Calculate mechanosorptive strain from incremental scheme\n",
    "def calculate_mechanosorptive_strain_from_inc(wdata, tdata, sdata, edata, w_cycle_idx, eps_cycle_idx, firstc_l=6, lastc_l=9, firstc_un=None, lastc_un=-1):\n",
    "    \n",
    "    #print(w_cycle_idx, len(w_cycle_idx), eps_cycle_idx, len(eps_cycle_idx))\n",
    "    ########## Calculate strain increments ##########\n",
    "    # Find strain templates\n",
    "    [t_cycle, eps_cycle, eps_cycle1] = analyze_cycles(tdata, wdata, edata, w_cycle_idx, eps_cycle_idx)#, firstc=firstc_l, lastc=lastc_l)\n",
    "\n",
    "    # Initialize arrays\n",
    "    seg_times = []\n",
    "    seg_vals = []\n",
    "    seg_moist = []\n",
    "    seg_stress = []\n",
    "    \n",
    "    for i, end in enumerate(w_cycle_idx):\n",
    "        # Set previous cycle\n",
    "        start = -1 if i == 0 else w_cycle_idx[i-1]\n",
    "        # Extract time\n",
    "        t_w = tdata[start+1:end+1]\n",
    "        t_w_rel = (t_w - t_w[0]) / (t_w[-1] - t_w[0])\n",
    "        # Extract moisture\n",
    "        w_w = wdata[start+1:end+1]\n",
    "        w_marks = UnivariateSpline(t_w_rel, w_w, k=5, s=0)(t_cycle)\n",
    "        seg_moist.append(w_marks)\n",
    "        # Extract stress\n",
    "        s_w = sdata[start+1:end+1]\n",
    "        s_marks = UnivariateSpline(t_w_rel, s_w, k=5, s=0)(t_cycle)\n",
    "        seg_stress.append(s_marks)\n",
    "    \n",
    "    for i, end in enumerate(eps_cycle_idx):\n",
    "        # Set previous cycle\n",
    "        start = -1 if i == 0 else eps_cycle_idx[i-1]\n",
    "        # Extract time\n",
    "        t_eps = tdata[start+1:end+1]\n",
    "        t_eps_rel = (t_eps - t_eps[0]) / (t_eps[-1] - t_eps[0])\n",
    "        # Extract strain\n",
    "        e_eps = edata[start+1:end+1]\n",
    "        e_marks = UnivariateSpline(t_eps_rel, e_eps, k=5, s=0)(t_cycle)\n",
    "        seg_vals.append(e_marks)\n",
    "\n",
    "    ########## Correct jump of template I (1st cycle) ##########\n",
    "    # Get corresponding moisture and time\n",
    "    w_end = seg_moist[0][-1]\n",
    "    w_temp = seg_moist[0][:len(seg_moist[0])//2]\n",
    "    idx = np.abs(w_temp - w_end).argmin()\n",
    "    t_start = t_cycle[idx]\n",
    "    idx_t = np.abs(t_cycle - t_start).argmin()\n",
    "    # Find delta strain\n",
    "    delta_end = eps_cycle[-1] - eps_cycle[idx_t]\n",
    "    x1, y1 = t_cycle[0], 0\n",
    "    x2, y2 = t_cycle[-1], delta_end\n",
    "    m0 = (y2 - y1) / (x2 - x1)\n",
    "    b0 = y1 - m0 * x1\n",
    "    # Shift jump by linear scaling\n",
    "    shift_cycle = m0*t_cycle + b0 \n",
    "    eps_cycle = eps_cycle - shift_cycle\n",
    "    ########## Calculate mechanosorp. strain of 1st cycle ##########\n",
    "    # Build temporary strain of first cycle\n",
    "    shift = eps_cycle[0] - edata[0] # shift to start at initial elastic strain (if not done before)\n",
    "    first_cycle = eps_cycle - shift\n",
    "    first_cycle_ms = seg_vals[0] - first_cycle\n",
    "\n",
    "    ########## Correct jump of template II (from 2nd cycle) ##########\n",
    "    # Find delta strain\n",
    "    delta_end = eps_cycle1[-1] - eps_cycle1[0]\n",
    "    x1, y1 = t_cycle[0], eps_cycle1[0] - eps_cycle1[0]\n",
    "    x2, y2 = t_cycle[-1], delta_end\n",
    "    m1 = (y2 - y1) / (x2 - x1)\n",
    "    b1 = y1 - m1 * x1\n",
    "    # Shift jump by linear scaling\n",
    "    shift_cycle = m1*t_cycle + b1\n",
    "    eps_cycle1 = eps_cycle1 - shift_cycle\n",
    "\n",
    "    ######### Calculate mechanosorp. strain of 1st cycle ##########\n",
    "    second_cycle = eps_cycle1 - eps_cycle1[0] # shift to start at 0 (if not done before)\n",
    "    seg_eps = []\n",
    "    seg_eps.append(first_cycle_ms)\n",
    "    for i in range(1,len(w_cycle_idx)):\n",
    "        #print(len(seg_vals[i]), len(second_cycle), seg_vals[i][0])\n",
    "        eps_inc = seg_vals[i] - (second_cycle + seg_vals[i][0]) + seg_eps[-1][-1]\n",
    "        seg_eps.append(eps_inc)\n",
    "    \n",
    "    \"\"\"plt.figure(figsize=(7,5))\n",
    "    plt.plot(t_cycle, first_cycle_ms, label=\"mcs1\")\n",
    "    plt.plot(t_cycle, eps_cycle, label=\"Theta 1\")\n",
    "    plt.plot(t_cycle, seg_vals[0], label=\"eps_red\")\n",
    "    # Extract time\n",
    "    t_eps = tdata[:eps_cycle_idx[0]+1]\n",
    "    t_eps_rel = (t_eps - t_eps[0]) / (t_eps[-1] - t_eps[0])\n",
    "    # Extract strain\n",
    "    e_eps = edata[:eps_cycle_idx[0]+1]\n",
    "    plt.plot(t_eps_rel, e_eps, label=\"eps_red_0\")\n",
    "    #plt.plot(t_cycle, eps_cycle1, label=\"Theta 2\")\n",
    "    plt.xlabel(\"t [s]\")\n",
    "    plt.ylabel(\"Strain ε [-]\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"if firstc_un is not None:\n",
    "        ########## Find stress drop ##########\n",
    "        # Find stress drop\n",
    "        stress_flat = np.concatenate(seg_stress)\n",
    "        drop_idx_flat = find_local_drop(stress_flat)\n",
    "        # Fit unloaded cycles\n",
    "        _, [t_end, eps_end] = analyze_cycles(tdata, wdata, edata, cycle_idx, firstc=firstc_un, lastc=lastc_un)\n",
    "        ########## Correct jump of template III (last cycle) ##########\n",
    "        delta_end = eps_end[-1] - eps_end[0]\n",
    "        x1, y1 = t_cycle[0], eps_end[0] - eps_end[0]\n",
    "        x2, y2 = t_cycle[-1], delta_end\n",
    "        m = (y2 - y1) / (x2 - x1)\n",
    "        b = y1 - m * x1\n",
    "        shift_cycle = m*t_cycle + b \n",
    "        eps_end = eps_end - shift_cycle\n",
    "        ########## Correct strain for unloading ##########\n",
    "        # Build temporary strain of last cycle\n",
    "        last_cycle = UnivariateSpline(t_end, eps_end, k=5, s=0)(seg_times[0])\n",
    "        shift = last_cycle[0]\n",
    "        last_cycle = last_cycle - shift\n",
    "\n",
    "        M = seg_stress[0].shape[0]\n",
    "        drop_cycle = drop_idx_flat // M\n",
    "        inc_vals = []\n",
    "        inc_vals.append(first_cycle_ms)\n",
    "        for i,d in enumerate(cycle_idx):\n",
    "            if i > 0:\n",
    "                # Evaluate cycle at given segmented times\n",
    "                idx = np.searchsorted(tdata, seg_times[i], side='left')\n",
    "                idx = np.clip(idx, 0, len(tdata)-1)\n",
    "                prev = np.clip(idx-1, 0, len(tdata)-1)\n",
    "                choose_prev = np.abs(tdata[prev] - seg_times[0]) <= np.abs(tdata[idx] - seg_times[0])\n",
    "                idx[choose_prev] = prev[choose_prev]\n",
    "                epsP = edata[idx]\n",
    "                \n",
    "                if i > drop_cycle:\n",
    "                    shift = last_cycle[0] - epsP[0]\n",
    "                    inc_vals.append(epsP - last_cycle + shift + inc_vals[-1][-1])\n",
    "\n",
    "                else:\n",
    "                    shift = second_cycle[0] - epsP[0]\n",
    "                    inc_vals.append(epsP - second_cycle + shift + inc_vals[-1][-1])\n",
    "\n",
    "        # Flatten strain array        \n",
    "        eps_flat  = np.concatenate(inc_vals)\"\"\"\n",
    "\n",
    "    # Re-sample time array\n",
    "    for i, end in enumerate(eps_cycle_idx):\n",
    "        # Set previous cycle\n",
    "        start = -1 if i == 0 else eps_cycle_idx[i-1]\n",
    "        # Extract time\n",
    "        t0 = tdata[start+1]\n",
    "        t1 = tdata[end]\n",
    "        seg_times.append(t_cycle * (t1 - t0) + t0)\n",
    "\n",
    "    return first_cycle, second_cycle, second_cycle, seg_eps, seg_times, seg_moist, seg_stress\n",
    "# end: def calculate_mechanosorptive_strain_from_inc\n",
    "\n",
    "# Fitting model of mechanosorptive strain\n",
    "def mechanosorptive_model(comp_j, stress, tdata, wdata):\n",
    "    \n",
    "    # Charachteristic moistures [-]\n",
    "    mu_0 = np.array([1., 10., 100.])/100.\n",
    "    # Build moisture‐rate array\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    dt = np.diff(tdata)\n",
    "    dw = np.diff(wdata)\n",
    "    wrate = np.concatenate(([0.], np.abs(dw) / dt))\n",
    "\n",
    "    #print(tdata, wrate, comp_j)\n",
    "    \n",
    "    # Initialize\n",
    "    eps_j  = np.zeros_like(mu_0)\n",
    "    eps_ms = np.zeros_like(wrate)\n",
    "    \n",
    "    # Calculate mechanosorptive strain\n",
    "    for i in range(1, len(tdata)):\n",
    "        delta_t = tdata[i] - tdata[i-1]\n",
    "        # Calculate mechanosorptive increment\n",
    "        deps = (wrate[i] / mu_0) * (comp_j * stress[i] - eps_j)\n",
    "        eps_j  += deps * delta_t\n",
    "        eps_ms[i] = eps_j.sum()\n",
    "\n",
    "    #print(eps_ms)\n",
    "    \n",
    "    return eps_ms\n",
    "# end: def mechanosorptive_model\n",
    "\n",
    "# Plot mechanosorptive strain vs moisture\n",
    "def calculate_mechanosorptive_strain(sample_type, w_arr, t_arr, s_arr, eps_arr, w_cycle_idx, eps_cycle_idx):\n",
    "\n",
    "    # Calculate strain components from total strain\n",
    "    eps_comp = calculate_strain_components(sample_type, w_arr, t_arr, s_arr, eps_arr)\n",
    "    ref_strain = eps_arr - eps_comp[5] # ref. strain = total - viscoel.\n",
    "    \n",
    "    \"\"\"plt.figure(figsize=(7,5))\n",
    "    plt.plot(eps_comp[0], eps_arr, label=\"mcs1\")\n",
    "    plt.plot(eps_comp[0], ref_strain, label=\"eps_red\")\n",
    "    plt.xlabel(\"t [s]\")\n",
    "    plt.ylabel(\"Strain ε [-]\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "    # Calculate mechanosorptive strain from incremental scheme\n",
    "    first_cycle, second_cycle , _, seg_eps, seg_times, seg_moist, seg_stress = calculate_mechanosorptive_strain_from_inc(eps_comp[1], eps_comp[0], eps_comp[2], ref_strain, w_cycle_idx, eps_cycle_idx)\n",
    "    exp_eps_ms  = np.concatenate(seg_eps)\n",
    "    times_flat  = np.concatenate(seg_times)\n",
    "    w_flat = np.concatenate(seg_moist)\n",
    "    s_flat = np.concatenate(seg_stress)\n",
    "\n",
    "    \"\"\"plt.figure(figsize=(8,8))\n",
    "    plt.plot(times_flat, exp_eps_ms, label=\"eps_ms\")\n",
    "    plt.plot(eps_comp[0], ref_strain, label=\"eps_red\")\n",
    "    \n",
    "    for i in range(len(eps_cycle_idx)):\n",
    "        if i == 0:\n",
    "            plt.plot(seg_times[0], first_cycle, label=\"template\", color='green')\n",
    "        else:\n",
    "            plt.plot(seg_times[i], second_cycle+ref_strain[eps_cycle_idx[i-1]], color='green')\n",
    "    \n",
    "    \n",
    "    for idx in eps_cycle_idx:\n",
    "        plt.axvline(x=t_arr[idx], color=\"grey\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"t [s]\")\n",
    "    plt.ylabel(\"Strain [-]\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "    # Fit incremental mechanosorptive strain\n",
    "    initial_guess = np.array([0.01]*3, dtype=float) # initial guess\n",
    "    # Residual function for least_squares\n",
    "    def residual(comp_j):\n",
    "        pred = mechanosorptive_model(comp_j, s_flat, times_flat, w_flat)\n",
    "        return (pred - exp_eps_ms)\n",
    "    # Bound compliances to be non‐negative\n",
    "    result = least_squares(residual, initial_guess, bounds=(0, np.inf), xtol=1e-8, ftol=1e-8)\n",
    "    comp_opt = result.x # get prony coefficients\n",
    "    eps_ms_fit = mechanosorptive_model(comp_opt, s_flat, times_flat, w_flat) # fitted strain\n",
    "\n",
    "    return times_flat, w_flat, s_flat, exp_eps_ms, eps_ms_fit, comp_opt\n",
    "# end: def plot_mechanos_strain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Prepare experimental datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## PREPARE EXPERIMENTAL DATASETS ##################\n",
    "# Load data from dvs test\n",
    "data = np.load(dvs_path)\n",
    "wdata0 = data['w'][0] # moisture of 1st cycle\n",
    "tdata0 = data['t'][0] # time of 1st cycle\n",
    "wdata = data['w'][1] # moisture of following cycles\n",
    "tdata = data['t'][1]  # time of following cycles\n",
    "\n",
    "\n",
    "# Retrieve sample types\n",
    "sample_types = get_sample_types(folder_path)[3:]  # keep RL, RT, TR (per your earlier setup)\n",
    "\n",
    "# Extract datasets\n",
    "dataset = []\n",
    "for sample_type in sample_types:\n",
    "\n",
    "    # Find experiments of sample type\n",
    "    matches = get_samples_by_type(sample_type, folder_path)\n",
    "\n",
    "    # Group by loading degree\n",
    "    grouped = defaultdict(list)\n",
    "    for exp, exp_path, sample_name in matches:\n",
    "        ld = np.round(extract_loading_deg(exp_path, sample_name), 1)\n",
    "        grouped[ld].append((exp, exp_path, sample_name))\n",
    "\n",
    "    # Build dataset entries for each loading degree and sample\n",
    "    for ld, group in grouped.items():\n",
    "        for idx, (exp, exp_path, sample_name) in enumerate(group):\n",
    "\n",
    "            # Prepare file paths and load raw data\n",
    "            file_folder = os.path.join(exp_path, \"Results\", sample_name)\n",
    "            strain_results = load_strain_struct(file_folder)\n",
    "\n",
    "            total_strain = strain_results['axial_strain']\n",
    "            time = strain_results['exp_duration']\n",
    "            stress = strain_results['stress_meas']\n",
    "\n",
    "            # Cut off pre-mechanosorption\n",
    "            cycle_idx = find_cycle_ends(strain_results['RH_mean'], tol=1, merge_gap=5)  # find cycle ends\n",
    "            t0 = cycle_idx[0]  # get first index = creep pre-mechanosorption\n",
    "            time = time[t0:] - time[t0]\n",
    "            stress = stress[t0:]\n",
    "            total_strain = total_strain[t0:] - total_strain[t0]\n",
    "            cycle_idx = cycle_idx[1:] - t0\n",
    "\n",
    "            # Determine load drop\n",
    "            num_cycles = len(cycle_idx)\n",
    "            if num_cycles > 8:\n",
    "                # Drop unloading cycles\n",
    "                guess_drop_idx = (cycle_idx[-2] + cycle_idx[-4]) / 2.0\n",
    "                drop_idx = find_local_drop(stress, stress_drop=guess_drop_idx, window=10)\n",
    "                idx_before, drop_idx, idx_after = locate_drop_idx(cycle_idx, drop_idx)\n",
    "                t_full, w_full, s_full = build_dynamic_cycles(\n",
    "                    tdata0, wdata0, tdata, wdata,\n",
    "                    time, cycle_idx, idx_before, drop_idx, [stress[0], stress[-1]]\n",
    "                )\n",
    "            else:\n",
    "                idx_before = drop_idx = idx_after = len(time) - 1\n",
    "                t_full, w_full, s_full = build_dynamic_cycles(\n",
    "                    tdata0, wdata0, tdata, wdata,\n",
    "                    time, cycle_idx, idx_before, drop_idx, [stress[0]]\n",
    "                )\n",
    "\n",
    "            # Find new cycles index\n",
    "            n_pre = np.searchsorted(cycle_idx, idx_before, side=\"right\")\n",
    "            cycle_len = len(tdata)\n",
    "            new_cycle_idx = [int((i + 1) * cycle_len - 1) for i in range(n_pre)]\n",
    "            if (sample_type == 'RT' and ld == 0.5 and idx == 4) or (sample_type == 'RT' and ld == 0.3 and idx in [1, 2, 3]):\n",
    "                new_cycle_idx = np.concatenate([new_cycle_idx[:-1], [len(t_full) - 1]])\n",
    "            else:\n",
    "                new_cycle_idx = np.concatenate([new_cycle_idx, [len(t_full) - 1]])\n",
    "\n",
    "            # Up-sample strain with a spline\n",
    "            spline = make_interp_spline(time, total_strain, k=5)\n",
    "            eps_full = spline(t_full)\n",
    "\n",
    "            # Find index of local minimum strains\n",
    "            min_indices, _ = find_peaks(-eps_full)\n",
    "            picked_minima, picked_anchors, distances = snap_minima_to_cycles(\n",
    "                min_indices, new_cycle_idx, max_gap=None, unique=True\n",
    "            )\n",
    "            picked_minima = np.unique(picked_minima)\n",
    "\n",
    "            # Cut last cycle out?\n",
    "            t_sel = np.asarray(t_full)[new_cycle_idx]\n",
    "            diffs = np.diff(t_sel)\n",
    "            prev = diffs[:-1]\n",
    "            last = diffs[-1]\n",
    "            baseline = np.median(prev) if len(prev) else last\n",
    "            tol = 0.5\n",
    "            if abs(last - baseline) > tol and len(new_cycle_idx) > 0:\n",
    "                new_cycle_idx = new_cycle_idx[:-1]\n",
    "\n",
    "            if len(picked_minima) < len(new_cycle_idx):\n",
    "                picked_minima = np.append(picked_minima, new_cycle_idx[-1])\n",
    "            picked_minima = np.sort(picked_minima)\n",
    "\n",
    "            # Add initial elastic strain\n",
    "            comp_el = fit_elastic_comp(elastic_path)\n",
    "            eps_el = calculate_elastic_strain(comp_el, sample_type, s_full, w_full)\n",
    "            eps_full = eps_full + eps_el[0]\n",
    "\n",
    "            # === calculate mechanosorptive strain and compliance values ===\n",
    "            t_flat, w_flat, s_flat, exp_eps_ms, eps_ms_fit, comp_opt = calculate_mechanosorptive_strain(\n",
    "                sample_type, w_full, t_full, s_full, eps_full, new_cycle_idx, picked_minima)\n",
    "\n",
    "            # ---- store only the metadata we need for Excel (so we don't have to reload later)\n",
    "            meta = {\n",
    "                'thickness'   : float(strain_results.get('thickness')),\n",
    "                'width'       : float(strain_results.get('width')),\n",
    "                'area'        : float(strain_results.get('area')),\n",
    "                'exp_duration': max(t_full),\n",
    "                'load_nom'    : float(strain_results.get('load_nom')),\n",
    "                'stress_nom'  : strain_results.get('load_nom')/strain_results.get('area'),\n",
    "                'load_meas'   : s_full[0] * strain_results.get('area'), #strain_results.get('load_meas'),\n",
    "                'stress_meas' : s_full[0], #strain_results.get('stress_meas'),\n",
    "                'loading_deg' : float(strain_results.get('loading_deg')),\n",
    "                'comp_opt'    : comp_opt,\n",
    "            }\n",
    "\n",
    "            # Append to dataset\n",
    "            dataset.append({\n",
    "                'exp'            : exp,\n",
    "                'sample_type'    : sample_type,\n",
    "                'sample_name'    : sample_name,\n",
    "                'ld'             : ld,\n",
    "                't_full'         : t_flat,\n",
    "                'w_full'         : w_flat,\n",
    "                's_full'         : s_flat,\n",
    "                'eps_full'       : exp_eps_ms,\n",
    "                'w_cycle_idx'    : new_cycle_idx,\n",
    "                'eps_cycle_idx'  : picked_minima,\n",
    "                'stress'         : stress,\n",
    "                'meta'           : meta,  # <— store meta for Excel writing\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Save excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(savepath, exist_ok=True)\n",
    "# ======================= EXCEL HELPERS =======================\n",
    "def _sanitize_sheet_name(name: str) -> str:\n",
    "    \"\"\"Make a valid Excel sheet name (<=31 chars and no \\ / * ? : [ ])\"\"\"\n",
    "    name = re.sub(r'[\\\\/*?:\\[\\]]', '_', str(name))\n",
    "    return name[:31] if len(name) > 31 else name\n",
    "\n",
    "def _dedupe(name: str, used: set) -> str:\n",
    "    \"\"\"Ensure sheet name uniqueness within a workbook.\"\"\"\n",
    "    base = name\n",
    "    i = 1\n",
    "    while name in used:\n",
    "        suffix = f\"_{i}\"\n",
    "        name = (base[: 31 - len(suffix)]) + suffix\n",
    "        i += 1\n",
    "    used.add(name)\n",
    "    return name\n",
    "\n",
    "def _meta_dataframe(entry: dict) -> pd.DataFrame:\n",
    "    \"\"\"Top two-column block as specified.\"\"\"\n",
    "    m = entry['meta']\n",
    "    comp_opt = m.get('comp_opt', [np.nan, np.nan, np.nan])\n",
    "    # Fixed Mu values\n",
    "    mu_vals = [1, 10, 100]\n",
    "\n",
    "    rows = [\n",
    "        (\"Experiment\",                  entry['exp']),\n",
    "        (\"Sample\",                      entry['sample_name']),\n",
    "        (\"Sample Type\",                 entry['sample_type']),\n",
    "        (\"Thickness [mm]\",              m.get('thickness')),\n",
    "        (\"Width [mm]\",                  m.get('width')),\n",
    "        (\"Area [mm^2]\",                 m.get('area')),\n",
    "        (\"Duration [h]\",                m.get('exp_duration')),\n",
    "        (\"Nominal Load [N]\",            m.get('load_nom')),\n",
    "        (\"Nominal Stress [MPa]\",        m.get('stress_nom')),\n",
    "        (\"Creep Load [N]\",              m.get('load_meas')),\n",
    "        (\"Creep Stress [MPa]\",          m.get('stress_meas')),\n",
    "        (\"Loading Degree [%]\",  m.get('loading_deg') * 100),\n",
    "        (\"Compliance_1 [1/GPa]\",        comp_opt[0]*1000 if len(comp_opt) > 0 else np.nan),\n",
    "        (\"Compliance_2 [1/GPa]\",        comp_opt[1]*1000 if len(comp_opt) > 0 else np.nan),\n",
    "        (\"Compliance_3 [1/GPa]\",        comp_opt[2]*1000 if len(comp_opt) > 0 else np.nan),\n",
    "        (\"Mu_1 [%]\",                    mu_vals[0]),\n",
    "        (\"Mu_2 [%]\",                    mu_vals[1]),\n",
    "        (\"Mu_3 [%]\",                    mu_vals[2]),\n",
    "    ]\n",
    "    return pd.DataFrame(rows, columns=[\"\", \"\"])\n",
    "\n",
    "# ======================= EXCEL WRITER =======================\n",
    "def save_excel_datasets(dataset: list, savepath: str):\n",
    "    # Group by sample_type\n",
    "    by_type = defaultdict(list)\n",
    "    for entry in dataset:\n",
    "        by_type[entry['sample_type']].append(entry)\n",
    "\n",
    "    for sample_type, entries in by_type.items():\n",
    "        out_file = os.path.join(savepath, f\"{sample_type}_samples_results.xlsx\")\n",
    "        used_sheet_names = set()\n",
    "\n",
    "        with pd.ExcelWriter(out_file, engine=\"openpyxl\") as writer:\n",
    "            for entry in entries:\n",
    "                # Top meta block\n",
    "                meta_df = _meta_dataframe(entry)\n",
    "\n",
    "                # Bottom table\n",
    "                t_full   = np.asarray(entry['t_full'])\n",
    "                w_full   = np.asarray(entry['w_full'])\n",
    "                s_full   = np.asarray(entry['s_full'])\n",
    "                eps_full = np.asarray(entry['eps_full'])\n",
    "\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    creep_comp = np.where(s_full != 0, (eps_full / s_full) * 1000.0, np.nan)\n",
    "\n",
    "                data_df = pd.DataFrame({\n",
    "                    \"Time [h]\":                  t_full,\n",
    "                    \"Moisture [%]\":              w_full*100,\n",
    "                    #\"Stress [MPa]\":              s_full,\n",
    "                    \"Creep Strain [-]\":          eps_full,\n",
    "                    \"Creep Compliance [1/GPa]\":  creep_comp,\n",
    "                })\n",
    "\n",
    "                # Sheet name = sample_name (sanitized & de-duplicated)\n",
    "                sheet_name = _dedupe(_sanitize_sheet_name(entry['sample_name']), used_sheet_names)\n",
    "\n",
    "                # Write meta at the top (A1), no headers\n",
    "                meta_df.to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=0, startcol=0)\n",
    "\n",
    "                # Leave one empty row, then write data table with headers\n",
    "                start_row = len(meta_df) + 2\n",
    "                data_df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=start_row, startcol=0)\n",
    "\n",
    "                # Basic column widths (openpyxl)\n",
    "                ws = writer.sheets[sheet_name]\n",
    "                ws.column_dimensions['A'].width = 22  # labels\n",
    "                ws.column_dimensions['B'].width = 28  # values\n",
    "                # Ensure data columns are readable\n",
    "                for col in ['A', 'B', 'C', 'D', 'E']:\n",
    "                    ws.column_dimensions[col].width = max(ws.column_dimensions[col].width or 0, 18)\n",
    "\n",
    "        print(f\"Saved: {out_file}\")\n",
    "\n",
    "\n",
    "# ====== USAGE ======\n",
    "# After your dataset-building loop finishes, just call:\n",
    "save_excel_datasets(dataset, savepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
