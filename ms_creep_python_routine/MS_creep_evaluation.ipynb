{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate incremental decomposition scheme for mechanosorptive strain  \n",
    "### A. Ferrara, May 2025  \n",
    "\n",
    "This notebook implements the incremental decomposition scheme proposed in [*Ferrara and Wittel (2025)*]() to isolate the mechanosorptive strain from the total strain measured with Digital Image Correlation (DIC). The analysis is applied to experimental results from **mechanosorptive creep tests** on **Norway spruce tissue slices** in the transverse directions, radial (R) and tangential (T), under cyclic relative humidity (RH) between 30% and 90%.  \n",
    "\n",
    "Samples were cut in the orientations **{RL, RT, TR}**, where the first letter denotes the longitudinal dimension and the second the transverse width, all comprising alternating bands of earlywood (EW) and latewood (LW). Tests were carried out at **different loading degrees (LD)**: {RL, RT} at 30% and 50%, and {TR} at 40% of the respective tensile strength. Reference tensile strengths were taken from our earlier work ([Ferrara and Wittel, 2024](https://doi.org/10.1515/hf-2024-0046)).  \n",
    "\n",
    "In the main folder, a subfolder must exist for each experiment (e.g., `MST30-9028`), containing:  \n",
    "- `MST30-9028_Data_new.txt` with sample details.  \n",
    "- A `Results` subfolder with one subfolder per sample, each containing a file `sample_name_strain_DIC.mat` generated at the end of the previous MATLAB analysis.  \n",
    "\n",
    "In addition, the following files (retrievable from **_MS_Creep_Tests_Dataset_** ([10.17632/rsrsw8h7mv.1]())) must be available in the main folder:  \n",
    "- **Tensile elastic tests**: mean elastic compliance values on Norway spruce tissue slices ([Ferrara and Wittel, 2024](https://doi.org/10.1515/hf-2024-0046)), enabling analyses of how creep compliance scales with elastic compliance.  \n",
    "- **Tensile creep tests**: element compliances of the Kelvin–Voigt model describing mean viscoelastic compliances ([Ferrara and Wittel, 2025a](https://doi.org/10.1007/s11043-025-09772-1)), serving as input for calculating the viscoelastic strain component.  \n",
    "- **Dynamic Vapor Sorption (DVS) tests**: an NPZ file with data from an RL-slice, following the RH profile applied in the mechanosorptive experiments.\n",
    "\n",
    "#### *A. Ferrara and F.K. Wittel \"Mechanosorptive Creep of Norway Spruce on the Tissue Scale Perpendicular to Grain\"*, *Holzforschung* (2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and working path\n",
    "Run the following section to import the required libraries and download the data folder in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import ast\n",
    "from mat73 import loadmat\n",
    "from math import *\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import lsq_linear\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import trim_mean\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "folder_path = '/home/aferrara/Desktop/Creep_experiments_1/'\n",
    "# Set file paths\n",
    "dvs_path = folder_path + 'dvs_data.npz'\n",
    "elastic_path = folder_path + 'elastic_compliances_Ferrara_Wittel_2024.csv'\n",
    "viscoel_path = folder_path + 'master_vec_prony_param.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized functions\n",
    "Run the following section to make the custom functions available for use in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve experiment path\n",
    "def get_rawdata_path(exp_code, main_path):\n",
    "    # Construct experiment path\n",
    "    exp_path = os.path.join(main_path, exp_code)\n",
    "    # Check if folder exists\n",
    "    if not os.path.isdir(exp_path): raise FileNotFoundError(f\"There is no folder for the experiment ({exp_path}).\")\n",
    "    # Check if the folder is empty\n",
    "    if not os.listdir(exp_path): raise FileNotFoundError(f\"The folder for the experiment exists but is empty ({exp_path}).\")\n",
    "    return exp_path\n",
    "# end: get_rawdata_path\n",
    "\n",
    "# Retrieve experiment data\n",
    "def get_experiment_data(exp_code, main_path):\n",
    "\n",
    "    # Get experiment folder path\n",
    "    exp_path = get_rawdata_path(exp_code, main_path)\n",
    "    data_path = os.path.join(exp_path, f\"{exp_code}_Data_new.txt\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(data_path): raise FileNotFoundError(f\"Experiment data file '{data_path}' not found.\")\n",
    "\n",
    "    # Read the data file into a Pandas DataFrame\n",
    "    df = pd.read_csv(data_path, delimiter='\\t')  # adjust delimiter if needed\n",
    "\n",
    "    # Convert DataFrame rows to a list of dictionaries\n",
    "    samples_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        sample = {\n",
    "            \"sample_holder\": row[0],    # Column 1\n",
    "            \"exp_name\": row[2],         # Column 3\n",
    "            \"img_name\": row[3],         # Column 4\n",
    "            \"thick\": row[10],           # Column 11\n",
    "            \"width\": row[11],           # Column 12\n",
    "            \"area\": row[10] * row[11],  # Column 11 * Column 12\n",
    "            \"load\": row[6],             # Column 7\n",
    "            \"failure_load\": row[7],     # Column 8\n",
    "            \"initial_load\": row[8],     # Column 9\n",
    "            \"load_cell\": row[9],        # Column 10\n",
    "            \"RH\": row[12]               # Column 13\n",
    "        }\n",
    "        samples_data.append(sample)\n",
    "\n",
    "    return samples_data, exp_path\n",
    "# end: def get_experiment_data\n",
    "\n",
    "# Retrieve all unique sample types across experiments\n",
    "def get_sample_types(main_path, prefix=\"MST30-90\"):\n",
    "    folders = sorted(\n",
    "        [d for d in os.listdir(main_path)\n",
    "         if os.path.isdir(os.path.join(main_path, d)) and d.startswith(prefix)])\n",
    "    types = set()\n",
    "    for exp in folders:\n",
    "        samples_data, _ = get_experiment_data(exp, main_path)\n",
    "        for sample in samples_data:\n",
    "            st = sample['exp_name'][3:5]\n",
    "            if st == 'LT':\n",
    "                tissue = \"EW\" if int(sample['exp_name'][-1]) >= 4 else \"LW\"\n",
    "                st = f\"LT-{tissue}\"\n",
    "            types.add(st)\n",
    "    return sorted(types)\n",
    "# end: def get_sample_types\n",
    "\n",
    "# Format a list of strings for printing\n",
    "def format_list(items):\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    if len(items) == 1:\n",
    "        return items[0]\n",
    "    if len(items) == 2:\n",
    "        return \" and \".join(items)\n",
    "    return \", \".join(items[:-1]) + \" and \" + items[-1]\n",
    "# end: def_format_llist\n",
    "\n",
    "# Load strain results of DIC analysis\n",
    "def load_strain_struct(grey_folder):\n",
    "    # Find the first matching .mat file\n",
    "    export_file_strain = glob.glob(os.path.join(grey_folder, '*_strain_DIC.mat'))[0]\n",
    "    if not os.path.isfile(export_file_strain):\n",
    "        raise FileNotFoundError(f\"The data struct {os.path.basename(export_file_strain)} does not exist in {grey_folder}.\")\n",
    "    # Load mat file\n",
    "    export_strain = loadmat(export_file_strain)['new_struct']\n",
    "    # Convert MATLAB serial datenum to Python datetime \n",
    "    time_name = ['imgtime', 'loadtime', 'RHtime']\n",
    "    for name in time_name:\n",
    "        matlab_datenum = export_strain[name]  # MATLAB serial number\n",
    "        if isinstance(matlab_datenum, np.ndarray):  # If it's an array, process each element\n",
    "            export_strain[name] = np.array([\n",
    "                (datetime.fromordinal(int(d)) + timedelta(days=d % 1) - timedelta(days=366)).replace(microsecond=0) for d in matlab_datenum])\n",
    "        else:  # If it's a single value, process directly\n",
    "            export_strain[name] = (datetime.fromordinal(int(matlab_datenum)) + timedelta(days=matlab_datenum % 1) - timedelta(days=366)).replace(microsecond=0)\n",
    "\n",
    "    return export_strain\n",
    "# end: def load_strain_struct\n",
    "\n",
    "# Locate previous and next index of drop \n",
    "def locate_drop_idx(cycle_idx, drop_idx, threshold=3):\n",
    "\n",
    "    # Compute distances to each cycle boundary\n",
    "    distances = np.abs(cycle_idx - drop_idx)\n",
    "    min_dist = distances.min()\n",
    "\n",
    "    if min_dist <= threshold:\n",
    "        # If drop too close, snap to the nearest cycle boundary\n",
    "        nearest = cycle_idx[np.argmin(distances)]\n",
    "        drop_idx = nearest\n",
    "        # Only look for the cycle boundary before drop\n",
    "        before = cycle_idx[cycle_idx < drop_idx]\n",
    "        before_idx = before.max() if before.size else None\n",
    "        after_idx = drop_idx\n",
    "    else:\n",
    "        # If drop is not too close, find both before and after\n",
    "        before = cycle_idx[cycle_idx < drop_idx]\n",
    "        after  = cycle_idx[cycle_idx > drop_idx]\n",
    "        before_idx = before.max() if before.size else None\n",
    "        after_idx  = after.min() if after.size  else None\n",
    "\n",
    "    return before_idx, drop_idx, after_idx\n",
    "# end: locate_drop_idx\n",
    "\n",
    "# Collect all samples matching a given sample type\n",
    "def get_samples_by_type(sample_type, main_path, prefix=\"MST30-90\"):\n",
    "    \n",
    "    matches = []\n",
    "    folders = sorted(\n",
    "        [d for d in os.listdir(main_path)\n",
    "         if os.path.isdir(os.path.join(main_path, d)) and d.startswith(prefix)])\n",
    "\n",
    "    for exp in folders:\n",
    "        samples_data, exp_path = get_experiment_data(exp, main_path)\n",
    "        for s in samples_data[::2]:\n",
    "            st = s['exp_name'][3:5]\n",
    "            if st == 'LT':\n",
    "                tissue = \"EW\" if int(s['exp_name'][-1]) >= 4 else \"LW\"\n",
    "                st = f\"LT-{tissue}\"\n",
    "            if st == sample_type:\n",
    "                matches.append((exp, exp_path, f\"{s['exp_name']}_{s['sample_holder']}\"))\n",
    "                \n",
    "    return matches\n",
    "# end: get_samples_by_type\n",
    "\n",
    "# Estract loading degree of a given sample\n",
    "def extract_loading_deg(exp_path, sample_name):\n",
    "\n",
    "    # Set path to image folder\n",
    "    file_folder = os.path.join(exp_path, \"Results\", f\"{sample_name}\")\n",
    "    # Check if folder exists\n",
    "    if not os.path.isdir(file_folder): raise FileNotFoundError(f\"The folder {file_folder} does not exist.\")\n",
    "    # Load data\n",
    "    strain_results = load_strain_struct(file_folder)\n",
    "\n",
    "    return strain_results['loading_deg']\n",
    "# end: def extract_loading_deg\n",
    "\n",
    "# Find indexes of local stress drop\n",
    "def find_local_drop(arr, stress_drop=None, window=5):\n",
    "\n",
    "    # Compute all differences\n",
    "    arr = np.asarray(arr)\n",
    "    n = arr.size\n",
    "    if n < 2: raise ValueError(\"Array must have at least 2 elements to find a drop.\")\n",
    "    diffs = np.diff(arr)\n",
    "    \n",
    "    if stress_drop is None:\n",
    "        # If no guess, then global search\n",
    "        return int(np.argmin(diffs)) + 1\n",
    "\n",
    "    # Clamp stress_drop to valid [0, n-1]\n",
    "    sd = int(np.clip(stress_drop, 0, n - 1))\n",
    "    # Convert to diffs‐index domain: diffs[j] = arr[j+1] - arr[j]\n",
    "    # so arr‐index i corresponds to diffs‐index j = i-1\n",
    "    j_center = sd - 1\n",
    "\n",
    "    # Define window in diffs‐space\n",
    "    j0 = max(0, j_center - window)\n",
    "    j1 = min(n - 2, j_center + window)\n",
    "\n",
    "    # Find drop\n",
    "    local = diffs[j0:j1+1]\n",
    "    if local.size:\n",
    "        j_drop = j0 + int(np.argmin(local))\n",
    "        return j_drop + 1\n",
    "\n",
    "    # If for some reason the window was empty,\n",
    "    # fall back to global search\n",
    "    return int(np.argmin(diffs)) + 1\n",
    "# end: def find_local_drop\n",
    "\n",
    "# Build identical moisture cycles with corresponding time and stress\n",
    "def build_cycles(tdata, wdata, total_cycles, stress_value, stressed_cycles):\n",
    "\n",
    "    # Convert to arrays\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    if tdata.shape != wdata.shape: raise ValueError(\"tdata and wdata must have the same shape\")\n",
    "\n",
    "    # Determine cycle period\n",
    "    period = tdata[-1] - tdata[0] + tdata[-1] - tdata[-2]\n",
    "\n",
    "    # Pre-allocate arrays\n",
    "    n_per_cycle = tdata.size\n",
    "    N = n_per_cycle * total_cycles\n",
    "    t_full = np.empty(N, dtype=tdata.dtype)\n",
    "    w_full = np.empty(N, dtype=wdata.dtype)\n",
    "    s_full = np.empty(N, dtype=float)\n",
    "\n",
    "    for cycle in range(total_cycles):\n",
    "        start = cycle * n_per_cycle\n",
    "        end = start + n_per_cycle\n",
    "\n",
    "        # Time offset\n",
    "        t_full[start:end] = tdata + cycle * period\n",
    "        w_full[start:end] = wdata\n",
    "\n",
    "        # Apply stress for initial cycles, else 0\n",
    "        if cycle < stressed_cycles:\n",
    "            s_full[start:end] = stress_value\n",
    "        elif cycle == stressed_cycles:\n",
    "            s_full[start-1] = 0.0\n",
    "            s_full[start:end] = 0.0\n",
    "        else:\n",
    "            s_full[start:end] = 0.0\n",
    "\n",
    "    return t_full, w_full, s_full\n",
    "# end: build_cycles\n",
    "\n",
    "# Build moisture cycles with different first sorption with corresponding time and stress\n",
    "def build_cycles_with_initial(tdata0, wdata0, tdata, wdata, total_cycles, stress_value, stressed_cycles):\n",
    "    \n",
    "    # Convert to arrays\n",
    "    t0 = np.asarray(tdata0)\n",
    "    w0 = np.asarray(wdata0)\n",
    "    t  = np.asarray(tdata)\n",
    "    w  = np.asarray(wdata)\n",
    "\n",
    "    # Determine cycle period\n",
    "    dt = t[-1] - t[-2]\n",
    "    period = (t[-1] - t[0]) + dt\n",
    "\n",
    "    # Total length\n",
    "    n0 = t0.size\n",
    "    n  = t.size\n",
    "    N  = n0 + n*(total_cycles-1)\n",
    "    # Pre-allocate\n",
    "    t_full = np.empty(N, dtype=t.dtype)\n",
    "    w_full = np.empty(N, dtype=w.dtype)\n",
    "    s_full = np.empty(N, dtype=float)\n",
    "\n",
    "    # Set first cycle\n",
    "    t_full[:n0] = t0\n",
    "    w_full[:n0] = w0\n",
    "    s_full[:n0] = stress_value if 0 < stressed_cycles else 0.0\n",
    "    # Set remaining cycles\n",
    "    for i in range(1, total_cycles):\n",
    "        start = n0 + (i-1)*n\n",
    "        end   = start + n\n",
    "        # Time shift\n",
    "        t_full[start:end] = t + (i-1)*period\n",
    "        w_full[start:end] = w\n",
    "        # Apply stress for initial cycles, else 0\n",
    "        if i < stressed_cycles:\n",
    "            s_full[start:end] = stress_value\n",
    "        elif i == stressed_cycles:\n",
    "            s_full[start-1] = 0.0\n",
    "            s_full[start:end] = 0.0\n",
    "        else:\n",
    "            s_full[start:end] = 0.0\n",
    "\n",
    "    return t_full, w_full, s_full\n",
    "# end: build_cycles_with_initial\n",
    "\n",
    "# Build moisture cycles with different first sorption from dvs data with corresponding time and stress\n",
    "def build_dynamic_cycles(tdata0, wdata0, tdata, wdata, t_exp, cycle_idx, idx_before, idx_drop, stress_val):\n",
    "    \n",
    "    # Inititalize arrays\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    if tdata.shape != wdata.shape: raise ValueError(\"tdata and wdata must have the same shape\")\n",
    "    \n",
    "    # Create loading cycles with first sorption different\n",
    "    n_pre = np.searchsorted(cycle_idx, idx_before,  side=\"right\") + 1 if len(stress_val) > 1 else np.searchsorted(cycle_idx, idx_before,  side=\"right\")\n",
    "    t_full, w_full, s_full = build_cycles_with_initial(tdata0, wdata0, tdata, wdata, n_pre, stress_val[0], n_pre)\n",
    "\n",
    "    if t_full[-1] > t_exp[idx_drop]:\n",
    "        idx_closest = np.abs(t_full - t_exp[idx_drop-2]).argmin()\n",
    "        s_full = s_full[:idx_closest+1]\n",
    "        t_full = t_full[:idx_closest+1]\n",
    "        w_full = w_full[:idx_closest+1]\n",
    "\n",
    "    return t_full, w_full, s_full\n",
    "# end: build_dynamic_cycles\n",
    "\n",
    "# Find end index of each moisture cycle\n",
    "def find_cycle_ends(w, tol=1e-5, merge_gap=5):\n",
    "    w = np.asarray(w)\n",
    "    w_min = w.min()\n",
    "\n",
    "    # Find all 'low plateau' indices\n",
    "    low_idxs = np.where(w <= (w_min + tol))[0]\n",
    "    if low_idxs.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Cluster them by index proximity\n",
    "    clusters = [[low_idxs[0]]]\n",
    "    for idx in low_idxs[1:]:\n",
    "        if idx - clusters[-1][-1] <= merge_gap:\n",
    "            # Same plateau cluster\n",
    "            clusters[-1].append(idx)\n",
    "        else:\n",
    "            # New plateau cluster\n",
    "            clusters.append([idx])\n",
    "\n",
    "    # Pick the largest index from each cluster\n",
    "    cycle_end_indices = [max(cluster) for cluster in clusters]\n",
    "\n",
    "    return cycle_end_indices\n",
    "# end: def find_cycle_ends\n",
    "\n",
    "# Convert RH into mean w (mean S/D of tissues from standard dvs test)\n",
    "def RH_to_w_mean(x):\n",
    "    w = 7.155e-11 * x**5 - 1.659e-08 * x**4 + 1.75e-06 * x**3 - 9.343e-05 * x**2 + 0.003795 * x + 0.002295\n",
    "    return w\n",
    "# end: RH_to_w_mean\n",
    "\n",
    "# Calculate hygroexpansion strain\n",
    "def calculate_hygroexp_strain(sample_type, wdata):\n",
    "\n",
    "    # Set hygroexpansion coefficients\n",
    "    alpha = {\"R\": 0.182,\n",
    "            \"T\": 0.343,\n",
    "            \"L\": 0.0061}\n",
    "    # Extract loading direction\n",
    "    long_dir = sample_type[0]\n",
    "    # Calculate hygroexpansion strain\n",
    "    eps_w = alpha[long_dir]*(wdata-wdata[0])\n",
    "\n",
    "    return eps_w\n",
    "# end: def calculate_hygroexp_strain\n",
    "\n",
    "# Fit elastic compliances to moisture\n",
    "def fit_elastic_comp(elastic_path):\n",
    "\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(elastic_path)\n",
    "    # Group and compute mean compliance\n",
    "    mean_df = df.rename(columns={'1/C0 [1/Mpa]': 'C0_mean'})\n",
    "    # Calculate average w for each RH\n",
    "    mean_df['w'] = RH_to_w_mean(mean_df['RH'])\n",
    "    # Fit quadratic function in w\n",
    "    el_fits = {}\n",
    "    for stype, sub in mean_df.groupby('sample_type'):\n",
    "        x = sub['w'].values\n",
    "        y = sub['C0_mean'].values\n",
    "        A = np.vstack(( x**2, x, np.ones_like(x) )).T\n",
    "        # choose bounds per sample_type\n",
    "        if stype in (\"LR\", \"LT-LW\"):\n",
    "            lower = [-np.inf, 0.0,     0.0]\n",
    "            upper = [ 0.0,     np.inf, np.inf]\n",
    "        else:\n",
    "            lower = [ 0.0,     0.0,     0.0]\n",
    "            upper = [ np.inf, np.inf, np.inf]\n",
    "        # Solve\n",
    "        res = lsq_linear(A, y, bounds=(lower, upper))\n",
    "        a, b, c = res.x\n",
    "        el_fits[stype] = (a, b, c)\n",
    "\n",
    "    return el_fits\n",
    "# end: fit_elastic_comp\n",
    "\n",
    "# Calculate elastic strain\n",
    "def calculate_elastic_strain(comp_el, sample_type, stress, wdata, load=True):\n",
    "\n",
    "    # Initialize array\n",
    "    N = len(wdata)\n",
    "    eps_el = [0.0] * N\n",
    "\n",
    "    if not load:\n",
    "        # All zeros if not loading\n",
    "        return eps_el\n",
    "\n",
    "    # Extract the three coefficients for compliance: a, b, c\n",
    "    a, b, c = comp_el[sample_type]\n",
    "\n",
    "    for i in range(N):\n",
    "        # Compute compliance at wdata[i]\n",
    "        C_i = a * wdata[i]**2 + b * wdata[i] + c\n",
    "        # Instantaneous elastic strain = compliance * stress\n",
    "        eps_el[i] = C_i * stress[i]\n",
    "\n",
    "    return eps_el\n",
    "# end: def calculate_elastic_strain\n",
    "\n",
    "# Fit viscoelastic compliances (prony coeff.) to moisture\n",
    "def fit_viscoelastic_comp(viscoel_path):\n",
    "\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(viscoel_path, sep=\",\", dtype={\"sample_type\": str, \"RH\": float, \"avg_comp_i\": str})\n",
    "    # Parse string‐list into float-list\n",
    "    df[\"comp_i\"] = df[\"avg_comp_i\"].apply(lambda s: [float(x) for x in ast.literal_eval(s)])\n",
    "    # Calculate average w for each RH\n",
    "    df[\"w\"] = RH_to_w_mean(df[\"RH\"]) \n",
    "    # Fit quadratic function in w\n",
    "    ve_fits = {}\n",
    "    for stype, sub in df.groupby(\"sample_type\"):\n",
    "        w_vals = sub[\"w\"].values\n",
    "        C = np.vstack(sub[\"comp_i\"].values)\n",
    "        polys = [np.polyfit(w_vals, C[:, j], 2) for j in range(C.shape[1])]\n",
    "        ve_fits[stype] = polys\n",
    "\n",
    "    return ve_fits\n",
    "# end: def fit_viscoelastic_comp\n",
    "\n",
    "# Calculate prony series\n",
    "def prony_response(comp_i, tdata, tau_0):\n",
    "    return (np.sum(comp_i) - np.sum(comp_i[:, None] * np.exp(-tdata / tau_0[:, None]), axis=0))\n",
    "# end: def prony_response\n",
    "\n",
    "# Calculate prony coefficients at w\n",
    "def comp_i_at_w(comp_ve, stype, wval):\n",
    "    polys = comp_ve[stype]\n",
    "    return np.array([ np.polyval(p, wval) for p in polys ])\n",
    "# end: def comp_i_at_w\n",
    "\n",
    "# Calculate viscoelastic creep compliance at given time and w\n",
    "def compliance_at_t_w(comp_ve, stype, tdata, wval, tau_0):\n",
    "    c_i = comp_i_at_w(comp_ve, stype, wval)\n",
    "    return prony_response(c_i, tdata, tau_0)\n",
    "# end: def compliance_at_t_w\n",
    "\n",
    "# Calculate viscoelastic strain\n",
    "def calculate_viscoelastic_strain(comp_ve, sample_type, stress, tdata, wdata, eps_el0):\n",
    "\n",
    "    # Set retardation times [h]\n",
    "    tau_0 = np.array([0.1, 1., 10., 100.])\n",
    "    # Calculate viscoelastic strain\n",
    "    eps_ve = []\n",
    "    eps_i = np.zeros(tau_0.shape)\n",
    "    eps_ve.append(eps_el0)\n",
    "    for i in range(1, len(tdata)):\n",
    "        # Calculate viscoelastic strain components\n",
    "        comp_i_n1 = comp_i_at_w(comp_ve, sample_type, wdata[i])\n",
    "        comp_i_n0 = comp_i_at_w(comp_ve, sample_type, wdata[i-1])\n",
    "        comp_i = (comp_i_n1 + comp_i_n0) /2.\n",
    "        deps_ve = np.zeros(tau_0.shape)\n",
    "        deps_ve = 1. / tau_0 * (comp_i * stress[i] - eps_i)\n",
    "        eps_i += deps_ve * (tdata[i]-tdata[i-1])\n",
    "        eps_ve.append(np.sum(eps_i)+eps_el0)\n",
    "\n",
    "    return eps_ve\n",
    "# end: def calculate_viscoelastic_strain\n",
    "\n",
    "# Calculate reference creep curves\n",
    "def calculate_ref_viscoel_curves(comp_ve, sample_type, stress, tdata, wdata, eps_el0):\n",
    "\n",
    "    # Set retardation times [h]\n",
    "    tau_0 = np.array([0.1, 1., 10., 100.])\n",
    "    # Calculate ref. curve at 30% RH = 0.07 mc\n",
    "    eps_ve_ref30 = compliance_at_t_w(comp_ve, sample_type, tdata, min(wdata), tau_0) * stress + eps_el0\n",
    "    # Calculate ref. curve at 90% RH = 0.20 mc\n",
    "    eps_ve_ref90 = compliance_at_t_w(comp_ve, sample_type, tdata, max(wdata), tau_0) * stress + eps_el0\n",
    "\n",
    "    return eps_ve_ref30, eps_ve_ref90\n",
    "# end: def calculate_ref_viscoel_curves\n",
    "\n",
    "# Fit monotonic regression\n",
    "def unimodal_fit(x, y):\n",
    "\n",
    "    # Initializations       \n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    N = len(y)\n",
    "    best_err = np.inf\n",
    "    best_fit = None\n",
    "\n",
    "    # Try each possible peak-location k\n",
    "    for k in range(1, N-1):\n",
    "        # Rising isotonic on [0..k]\n",
    "        ir_inc = IsotonicRegression(increasing=True)\n",
    "        y_inc = ir_inc.fit_transform(x[:k+1], y[:k+1])\n",
    "\n",
    "        # Falling  isotonic on [k..N-1]\n",
    "        ir_dec = IsotonicRegression(increasing=False)\n",
    "        y_dec = ir_dec.fit_transform(x[k:], y[k:])\n",
    "\n",
    "        # Stitch (avoid doubling the k-th point)\n",
    "        y_fit = np.concatenate([y_inc[:-1], y_dec])\n",
    "        err = np.sum((y - y_fit)**2)\n",
    "        if err < best_err:\n",
    "            best_err  = err\n",
    "            best_fit  = y_fit\n",
    "\n",
    "    return best_fit\n",
    "# end: def unimodal_fit\n",
    "\n",
    "# Compute incremental strains across cycles\n",
    "def compute_segment_deltas(tdata, edata, wdata, sdata, w_cycle_idx, n_segments=1000):\n",
    "\n",
    "    # Initialize arrays\n",
    "    seg_times = []\n",
    "    seg_vals = []\n",
    "    seg_stress = []\n",
    "    seg_moist = []\n",
    "    \n",
    "    for i, end in enumerate(w_cycle_idx):\n",
    "        # Set previous cycle\n",
    "        start = -1 if i == 0 else w_cycle_idx[i-1]\n",
    "        # Extract time\n",
    "        t_c = tdata[start+1:end+1]\n",
    "        rel = t_c - t_c[0]\n",
    "        markers = np.linspace(0, rel[-1], n_segments)\n",
    "        t_marks = markers + t_c[0]\n",
    "        seg_times.append(t_marks)\n",
    "        # Extract stress\n",
    "        s_c = sdata[start+1:end+1]\n",
    "        s_marks = UnivariateSpline(t_c, s_c, k=5, s=0)(t_marks)\n",
    "        seg_stress.append(s_marks)\n",
    "        # Extract moisture\n",
    "        w_c = wdata[start+1:end+1]\n",
    "        w_marks = UnivariateSpline(t_c, w_c, k=5, s=0)(t_marks)\n",
    "        seg_moist.append(w_marks)\n",
    "        # Extract strain\n",
    "        e_c = edata[start+1:end+1]\n",
    "        e_marks = UnivariateSpline(t_c, e_c, k=5, s=0)(t_marks)\n",
    "        seg_vals.append(e_marks)\n",
    "\n",
    "    # Convert to arrays\n",
    "    seg_times = np.array(seg_times)\n",
    "    seg_vals = np.array(seg_vals)\n",
    "    seg_moist = np.array(seg_moist)\n",
    "    seg_stress = np.array(seg_stress)\n",
    "\n",
    "    # Calculate delta strain\n",
    "    delta_s = seg_vals[1:] - seg_vals[:-1]\n",
    "\n",
    "    return seg_times, delta_s, seg_moist, seg_stress\n",
    "# end: compute_segment_deltas\n",
    "\n",
    "# Overlap and fit cycles\n",
    "def analyze_cycles(tdata, wdata, edata, w_cycle_idx, eps_cycle_idx, firstc=3, lastc=7, num_grid_points=1000):\n",
    "\n",
    "    # Convert to arrays\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    edata = np.asarray(edata)\n",
    "\n",
    "    # Compute relative indices and segment boundaries\n",
    "    firstc = firstc-2\n",
    "    if lastc!=-1:\n",
    "        lastc = lastc - 1\n",
    "        w_cycle_idx = w_cycle_idx[firstc:lastc+1]\n",
    "        eps_cycle_idx = eps_cycle_idx[firstc:lastc+1]\n",
    "    else:\n",
    "        w_cycle_idx = w_cycle_idx[firstc:]\n",
    "        eps_cycle_idx = eps_cycle_idx[firstc:]\n",
    "    w_segments = list(zip(w_cycle_idx, w_cycle_idx[1:]))\n",
    "    eps_segments = list(zip(eps_cycle_idx, eps_cycle_idx[1:]))\n",
    "\n",
    "    # Collect and shift cycles\n",
    "    cycles = []\n",
    "    for (w_start, w_end), (eps_start, eps_end) in zip(w_segments, eps_segments):\n",
    "        # W segment and its relative time (0..1)\n",
    "        t_w = tdata[w_start+1:w_end+1]\n",
    "        w_seg = wdata[w_start+1:w_end+1]\n",
    "        t_w_rel = (t_w - t_w[0]) / (t_w[-1] - t_w[0])\n",
    "        # E segment and its relative time (0..1)\n",
    "        t_e = tdata[eps_start+1:eps_end+1]\n",
    "        e_seg = edata[eps_start+1:eps_end+1]\n",
    "        t_e_rel = (t_e - t_e[0]) / (t_e[-1] - t_e[0])\n",
    "        # Interpolate strain on the w grid\n",
    "        e_on_w = np.interp(t_w_rel, t_e_rel, e_seg)\n",
    "        # Locate peak\n",
    "        peak_i = np.argmax(e_on_w)\n",
    "        peak_val = e_on_w[peak_i]\n",
    "        # Store cycle data\n",
    "        cycles.append({\n",
    "            't_seg_rel': t_w_rel,\n",
    "            'w':       w_seg,\n",
    "            'eps':       e_on_w,\n",
    "            'last_eps':  e_on_w[-1],\n",
    "            'peak_eps':  peak_val})\n",
    "\n",
    "    ######## BUILD TEMPLATE FOR ALL CYCLES (except 1) ########\n",
    "    # Use first cycle's peak as reference\n",
    "    ref_peak = cycles[0]['peak_eps']\n",
    "    # Apply vertical shift based on peaks\n",
    "    for c in cycles: c['eps_shifted'] = c['eps'] - c['peak_eps'] + ref_peak\n",
    "    # Build common time grid where all shifted cycles overlap\n",
    "    t_fit = np.linspace(0, 1, num_grid_points)\n",
    "\n",
    "    # Interpolate each shifted cycle onto the common grid\n",
    "    s_interp = np.vstack([c['eps_shifted'] for c in cycles])\n",
    "    avg_s_time = np.mean(s_interp, axis=0)\n",
    "    avg_s_time = avg_s_time - avg_s_time[0]\n",
    "\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(t_w_rel, avg_s_time, label=\"Mean\", marker='o', color='blue')\n",
    "    \"\"\"\n",
    "\n",
    "    s_interp = np.vstack([UnivariateSpline(c['t_seg_rel'], c['eps_shifted'], k=5, s=0.00001)(t_fit) for c in cycles])\n",
    "    # Calculate average strain\n",
    "    avg_s_time = np.mean(s_interp, axis=0)\n",
    "    # Shift template to start from second cycle\n",
    "    shift = avg_s_time[0] - edata[eps_cycle_idx[0]+1]\n",
    "    avg_s_time = avg_s_time - shift\n",
    "\n",
    "    # Try to fit avoiding any possible final increase\n",
    "    if lastc != -1:       \n",
    "        # Fit template\n",
    "        y_fit = unimodal_fit(t_fit, avg_s_time)\n",
    "        x = t_fit\n",
    "        peak_idx = np.argmax(y_fit)\n",
    "        # Fit a strictly‐decreasing isotonic to the tail\n",
    "        ir_down = IsotonicRegression(increasing=False)\n",
    "        x_tail = x[peak_idx:]\n",
    "        y_tail = y_fit[peak_idx:]\n",
    "        y_tail_iso = ir_down.fit_transform(x_tail, y_tail)\n",
    "        # Stitch it back together\n",
    "        avg_s_time = np.concatenate([y_fit[:peak_idx], y_tail_iso])\n",
    "\n",
    "\n",
    "    ######## BUILD TEMPLATE FOR 1st CYCLE ########\n",
    "    # Compute delta sorption\n",
    "    w0 = wdata[:w_cycle_idx[0]+1]\n",
    "    w1 = wdata[w_cycle_idx[0]+1:w_cycle_idx[1]+1]\n",
    "    idx1 = w1.argmax()\n",
    "    shift = w1[idx1] - w0[idx1]\n",
    "    w1 = w1 - shift\n",
    "    delta_w = [x/y for x, y in zip(w0, w1)]\n",
    "\n",
    "    # Pick target times\n",
    "    t_rel = (tdata[w_cycle_idx[0]+1:w_cycle_idx[1]+1] - tdata[w_cycle_idx[0]+1]) / (tdata[w_cycle_idx[1]+1] - tdata[w_cycle_idx[0]+1])\n",
    "    t0_rel = t_rel[idx1]\n",
    "    idx_fit = np.searchsorted(t_fit, t0_rel)\n",
    "    slice_times = t_fit[:idx_fit+1]\n",
    "    # Calculate scaling factor by interpolating delta_w\n",
    "    scale = np.interp(slice_times, t_rel[:idx1+1], delta_w[:idx1+1])\n",
    "    # Scale strain  proportional to sorption\n",
    "    avg_s_time0 = np.concatenate((avg_s_time[:idx_fit+1] * scale, avg_s_time[idx_fit+1:]), axis=0)\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.plot(t_fit, avg_s_time, label=\"Spline\", color='red')\n",
    "    plt.scatter(t_rel[idx1], avg_s_time[idx_fit], color='black', s = 50)\n",
    "    plt.plot(t_fit, avg_s_time0, label=\"Scale\", color='cyan')\n",
    "    plt.xlabel(\"t [s]\")\n",
    "    plt.ylabel(\"Strain ε [-]\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    # Shift template I to start from elastic strain\n",
    "    shift = avg_s_time0[0] - edata[0]\n",
    "    avg_s_time0 = avg_s_time0 - shift\n",
    "\n",
    "    # Shift template II to start from 0\n",
    "    shift = avg_s_time[0]\n",
    "    avg_s_time = avg_s_time - shift\n",
    "    \n",
    "    return [t_fit, avg_s_time0, avg_s_time]\n",
    "# end: def analyze_cycles\n",
    "\n",
    "# Calculate mechanosorptive strain from incremental scheme\n",
    "def calculate_mechanosorptive_strain_from_inc(wdata, tdata, sdata, edata, w_cycle_idx, eps_cycle_idx, firstc_l=6, lastc_l=9, firstc_un=None, lastc_un=-1):\n",
    "    \n",
    "    #print(w_cycle_idx, len(w_cycle_idx), eps_cycle_idx, len(eps_cycle_idx))\n",
    "    ########## Calculate strain increments ##########\n",
    "    # Find strain templates\n",
    "    [t_cycle, eps_cycle, eps_cycle1] = analyze_cycles(tdata, wdata, edata, w_cycle_idx, eps_cycle_idx)#, firstc=firstc_l, lastc=lastc_l)\n",
    "\n",
    "    # Initialize arrays\n",
    "    seg_times = []\n",
    "    seg_vals = []\n",
    "    seg_moist = []\n",
    "    seg_stress = []\n",
    "    \n",
    "    for i, end in enumerate(w_cycle_idx):\n",
    "        # Set previous cycle\n",
    "        start = -1 if i == 0 else w_cycle_idx[i-1]\n",
    "        # Extract time\n",
    "        t_w = tdata[start+1:end+1]\n",
    "        t_w_rel = (t_w - t_w[0]) / (t_w[-1] - t_w[0])\n",
    "        # Extract moisture\n",
    "        w_w = wdata[start+1:end+1]\n",
    "        w_marks = UnivariateSpline(t_w_rel, w_w, k=5, s=0)(t_cycle)\n",
    "        seg_moist.append(w_marks)\n",
    "        # Extract stress\n",
    "        s_w = sdata[start+1:end+1]\n",
    "        s_marks = UnivariateSpline(t_w_rel, s_w, k=5, s=0)(t_cycle)\n",
    "        seg_stress.append(s_marks)\n",
    "    \n",
    "    for i, end in enumerate(eps_cycle_idx):\n",
    "        # Set previous cycle\n",
    "        start = -1 if i == 0 else eps_cycle_idx[i-1]\n",
    "        # Extract time\n",
    "        t_eps = tdata[start+1:end+1]\n",
    "        t_eps_rel = (t_eps - t_eps[0]) / (t_eps[-1] - t_eps[0])\n",
    "        # Extract strain\n",
    "        e_eps = edata[start+1:end+1]\n",
    "        e_marks = UnivariateSpline(t_eps_rel, e_eps, k=5, s=0)(t_cycle)\n",
    "        seg_vals.append(e_marks)\n",
    "\n",
    "    ########## Correct jump of template I (1st cycle) ##########\n",
    "    # Get corresponding moisture and time\n",
    "    w_end = seg_moist[0][-1]\n",
    "    w_temp = seg_moist[0][:len(seg_moist[0])//2]\n",
    "    idx = np.abs(w_temp - w_end).argmin()\n",
    "    t_start = t_cycle[idx]\n",
    "    idx_t = np.abs(t_cycle - t_start).argmin()\n",
    "    # Find delta strain\n",
    "    delta_end = eps_cycle[-1] - eps_cycle[idx_t]\n",
    "    x1, y1 = t_cycle[0], 0\n",
    "    x2, y2 = t_cycle[-1], delta_end\n",
    "    m0 = (y2 - y1) / (x2 - x1)\n",
    "    b0 = y1 - m0 * x1\n",
    "    # Shift jump by linear scaling\n",
    "    shift_cycle = m0*t_cycle + b0 \n",
    "    eps_cycle = eps_cycle - shift_cycle\n",
    "    ########## Calculate mechanosorp. strain of 1st cycle ##########\n",
    "    # Build temporary strain of first cycle\n",
    "    shift = eps_cycle[0] - edata[0] # shift to start at initial elastic strain (if not done before)\n",
    "    first_cycle = eps_cycle - shift\n",
    "    first_cycle_ms = seg_vals[0] - first_cycle\n",
    "\n",
    "    ########## Correct jump of template II (from 2nd cycle) ##########\n",
    "    # Find delta strain\n",
    "    delta_end = eps_cycle1[-1] - eps_cycle1[0]\n",
    "    x1, y1 = t_cycle[0], eps_cycle1[0] - eps_cycle1[0]\n",
    "    x2, y2 = t_cycle[-1], delta_end\n",
    "    m1 = (y2 - y1) / (x2 - x1)\n",
    "    b1 = y1 - m1 * x1\n",
    "    # Shift jump by linear scaling\n",
    "    shift_cycle = m1*t_cycle + b1\n",
    "    eps_cycle1 = eps_cycle1 - shift_cycle\n",
    "\n",
    "    ######### Calculate mechanosorp. strain of 1st cycle ##########\n",
    "    second_cycle = eps_cycle1 - eps_cycle1[0] # shift to start at 0 (if not done before)\n",
    "    seg_eps = []\n",
    "    seg_eps.append(first_cycle_ms)\n",
    "    for i in range(1,len(w_cycle_idx)):\n",
    "        #print(len(seg_vals[i]), len(second_cycle), seg_vals[i][0])\n",
    "        eps_inc = seg_vals[i] - (second_cycle + seg_vals[i][0]) + seg_eps[-1][-1]\n",
    "        seg_eps.append(eps_inc)\n",
    "    \n",
    "    \"\"\"plt.figure(figsize=(7,5))\n",
    "    plt.plot(t_cycle, first_cycle_ms, label=\"mcs1\")\n",
    "    plt.plot(t_cycle, eps_cycle, label=\"Theta 1\")\n",
    "    plt.plot(t_cycle, seg_vals[0], label=\"eps_red\")\n",
    "    # Extract time\n",
    "    t_eps = tdata[:eps_cycle_idx[0]+1]\n",
    "    t_eps_rel = (t_eps - t_eps[0]) / (t_eps[-1] - t_eps[0])\n",
    "    # Extract strain\n",
    "    e_eps = edata[:eps_cycle_idx[0]+1]\n",
    "    plt.plot(t_eps_rel, e_eps, label=\"eps_red_0\")\n",
    "    #plt.plot(t_cycle, eps_cycle1, label=\"Theta 2\")\n",
    "    plt.xlabel(\"t [s]\")\n",
    "    plt.ylabel(\"Strain ε [-]\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"if firstc_un is not None:\n",
    "        ########## Find stress drop ##########\n",
    "        # Find stress drop\n",
    "        stress_flat = np.concatenate(seg_stress)\n",
    "        drop_idx_flat = find_local_drop(stress_flat)\n",
    "        # Fit unloaded cycles\n",
    "        _, [t_end, eps_end] = analyze_cycles(tdata, wdata, edata, cycle_idx, firstc=firstc_un, lastc=lastc_un)\n",
    "        ########## Correct jump of template III (last cycle) ##########\n",
    "        delta_end = eps_end[-1] - eps_end[0]\n",
    "        x1, y1 = t_cycle[0], eps_end[0] - eps_end[0]\n",
    "        x2, y2 = t_cycle[-1], delta_end\n",
    "        m = (y2 - y1) / (x2 - x1)\n",
    "        b = y1 - m * x1\n",
    "        shift_cycle = m*t_cycle + b \n",
    "        eps_end = eps_end - shift_cycle\n",
    "        ########## Correct strain for unloading ##########\n",
    "        # Build temporary strain of last cycle\n",
    "        last_cycle = UnivariateSpline(t_end, eps_end, k=5, s=0)(seg_times[0])\n",
    "        shift = last_cycle[0]\n",
    "        last_cycle = last_cycle - shift\n",
    "\n",
    "        M = seg_stress[0].shape[0]\n",
    "        drop_cycle = drop_idx_flat // M\n",
    "        inc_vals = []\n",
    "        inc_vals.append(first_cycle_ms)\n",
    "        for i,d in enumerate(cycle_idx):\n",
    "            if i > 0:\n",
    "                # Evaluate cycle at given segmented times\n",
    "                idx = np.searchsorted(tdata, seg_times[i], side='left')\n",
    "                idx = np.clip(idx, 0, len(tdata)-1)\n",
    "                prev = np.clip(idx-1, 0, len(tdata)-1)\n",
    "                choose_prev = np.abs(tdata[prev] - seg_times[0]) <= np.abs(tdata[idx] - seg_times[0])\n",
    "                idx[choose_prev] = prev[choose_prev]\n",
    "                epsP = edata[idx]\n",
    "                \n",
    "                if i > drop_cycle:\n",
    "                    shift = last_cycle[0] - epsP[0]\n",
    "                    inc_vals.append(epsP - last_cycle + shift + inc_vals[-1][-1])\n",
    "\n",
    "                else:\n",
    "                    shift = second_cycle[0] - epsP[0]\n",
    "                    inc_vals.append(epsP - second_cycle + shift + inc_vals[-1][-1])\n",
    "\n",
    "        # Flatten strain array        \n",
    "        eps_flat  = np.concatenate(inc_vals)\"\"\"\n",
    "\n",
    "    # Re-sample time array\n",
    "    for i, end in enumerate(eps_cycle_idx):\n",
    "        # Set previous cycle\n",
    "        start = -1 if i == 0 else eps_cycle_idx[i-1]\n",
    "        # Extract time\n",
    "        t0 = tdata[start+1]\n",
    "        t1 = tdata[end]\n",
    "        seg_times.append(t_cycle * (t1 - t0) + t0)\n",
    "\n",
    "    return first_cycle, second_cycle, second_cycle, seg_eps, seg_times, seg_moist, seg_stress\n",
    "# end: def calculate_mechanosorptive_strain_from_inc\n",
    "\n",
    "# Fitting model of mechanosorptive strain\n",
    "def mechanosorptive_model(comp_j, stress, tdata, wdata):\n",
    "    \n",
    "    # Charachteristic moistures [-]\n",
    "    mu_0 = np.array([1., 10., 100.])/100.\n",
    "    # Build moisture‐rate array\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    dt = np.diff(tdata)\n",
    "    dw = np.diff(wdata)\n",
    "    wrate = np.concatenate(([0.], np.abs(dw) / dt))\n",
    "    \n",
    "    # Initialize\n",
    "    eps_j  = np.zeros_like(mu_0)\n",
    "    eps_ms = np.zeros_like(wrate)\n",
    "\n",
    "    # Calculate mechanosorptive strain\n",
    "    for i in range(1, len(tdata)):\n",
    "        delta_t = tdata[i] - tdata[i-1]\n",
    "        # Calculate mechanosorptive increment\n",
    "        deps = (wrate[i] / mu_0) * (comp_j * stress[i] - eps_j)\n",
    "        eps_j  += deps * delta_t\n",
    "        eps_ms[i] = eps_j.sum()\n",
    "    \n",
    "    return eps_ms\n",
    "# end: def mechanosorptive_model\n",
    "\n",
    "# Plot mechanosorptive strain vs moisture\n",
    "def calculate_mechanosorptive_strain(sample_type, w_arr, t_arr, s_arr, eps_arr, w_cycle_idx, eps_cycle_idx):\n",
    "\n",
    "    # Calculate strain components from total strain\n",
    "    eps_comp = calculate_strain_components(sample_type, w_arr, t_arr, s_arr, eps_arr)\n",
    "    ref_strain = eps_arr - eps_comp[5] # ref. strain = total - viscoel.\n",
    "\n",
    "    # Calculate mechanosorptive strain from incremental scheme\n",
    "    first_cycle, second_cycle , _, seg_eps, seg_times, seg_moist, seg_stress = calculate_mechanosorptive_strain_from_inc(eps_comp[1], eps_comp[0], eps_comp[2], ref_strain, w_cycle_idx, eps_cycle_idx)\n",
    "    exp_eps_ms  = np.concatenate(seg_eps)\n",
    "    times_flat  = np.concatenate(seg_times)\n",
    "    w_flat = np.concatenate(seg_moist)\n",
    "    s_flat = np.concatenate(seg_stress)\n",
    "\n",
    "    # Fit incremental mechanosorptive strain\n",
    "    initial_guess = np.array([0.01]*3, dtype=float) # initial guess\n",
    "    # Residual function for least_squares\n",
    "    def residual(comp_j):\n",
    "        pred = mechanosorptive_model(comp_j, s_flat, times_flat, w_flat)\n",
    "        return (pred - exp_eps_ms)\n",
    "    # Bound compliances to be non‐negative\n",
    "    result = least_squares(residual, initial_guess, bounds=(0, np.inf), xtol=1e-8, ftol=1e-8)\n",
    "    comp_opt = result.x # get prony coefficients\n",
    "    eps_ms_fit = mechanosorptive_model(comp_opt, s_flat, times_flat, w_flat) # fitted strain\n",
    "\n",
    "    return times_flat, w_flat, s_flat, exp_eps_ms, eps_ms_fit, comp_opt\n",
    "# end: def plot_mechanos_strain\n",
    "\n",
    "# Calculate strain components (hygroexpansion, elastic, viscoelastic, mechanosorptive)\n",
    "def calculate_strain_components(sample_type, wdata, tdata, sdata, edata):\n",
    "\n",
    "    ########## Calculate hygroexpansion strain ##########\n",
    "    hygroexp_strain = calculate_hygroexp_strain(sample_type, wdata)\n",
    "\n",
    "    ########## Calculate elastic strain ##########\n",
    "    comp_el = fit_elastic_comp(elastic_path)\n",
    "    elastic_strain = calculate_elastic_strain(comp_el, sample_type, sdata, wdata)\n",
    "\n",
    "    ########## Calculate viscoelastic strain ##########\n",
    "    comp_ve = fit_viscoelastic_comp(viscoel_path)\n",
    "    viscoel_strain = calculate_viscoelastic_strain(comp_ve, sample_type, sdata, tdata, wdata, 0)\n",
    "    viscoel_ref30, viscoel_ref90 = calculate_ref_viscoel_curves(comp_ve, sample_type, sdata[0], tdata, wdata, 0)\n",
    "    \n",
    "    ########## Calculate mechanosorptive strain ##########\n",
    "    combo_full = hygroexp_strain + elastic_strain + viscoel_strain\n",
    "    mechanos_strain = edata - combo_full\n",
    "\n",
    "    return [tdata, wdata, sdata, hygroexp_strain, elastic_strain, viscoel_strain, viscoel_ref30, viscoel_ref90, mechanos_strain, edata]\n",
    "# end: def calculate_strains_component\n",
    "\n",
    "# Build colormaps for loading degrees\n",
    "def build_ld_colormaps(data, cmap_dict, scalar_range=(0.4, 0.9)):\n",
    "\n",
    "    if cmap_dict is None: cmap_dict = {0.5: cm.Purples, 0.4: cm.Greens, 0.3: cm.Oranges}\n",
    "    fallback_cmap = cm.Greys\n",
    "\n",
    "    # Group datasets by LD\n",
    "    grouped = defaultdict(list)\n",
    "    for entry in data: grouped[entry['ld']].append(entry)\n",
    "\n",
    "    # Generate list of colors for each group\n",
    "    start, end = scalar_range\n",
    "    colors_by_ld = {}\n",
    "    rep_colors = {}\n",
    "\n",
    "    # Sort by LD for deterministic behavior\n",
    "    for ld_value in sorted(grouped.keys()):\n",
    "        entries = grouped[ld_value]\n",
    "        cmap = cmap_dict.get(ld_value, fallback_cmap)\n",
    "        n = len(entries)\n",
    "\n",
    "        if n > 1: scalars = np.linspace(start, end, n)\n",
    "        else: scalars = [(start + end) / 2.0]\n",
    "\n",
    "        cols = [cmap(s) for s in scalars]\n",
    "        colors_by_ld[ld_value] = cols\n",
    "\n",
    "        # Choose a representative color:\n",
    "        # - third from the end if available, else the last available\n",
    "        rep_idx = n - 3 if n >= 3 else n - 1\n",
    "        rep_colors[ld_value] = cols[rep_idx]\n",
    "\n",
    "    return colors_by_ld, rep_colors\n",
    "# end: def build_ld_colormaps\n",
    "\n",
    "# Set shared grid between primary and secondary y-axis\n",
    "def set_shared_grid(ax1, ax2):\n",
    "\n",
    "    # Grab the current primary ticks\n",
    "    primary_ticks = ax1.get_yticks()\n",
    "    # Keep the same gridlines at the primary_ticks\n",
    "    ax1.set_yticks(primary_ticks)\n",
    "    # Compute secondary ticks by slicing its data‐range into len(primary_ticks) points\n",
    "    smin, smax = ax2.get_ylim()\n",
    "    secondary_ticks = np.linspace(smin, smax, len(primary_ticks))\n",
    "    ax2.set_yticks(secondary_ticks)\n",
    "    ax2.set_yticklabels([str(round(t,3)) for t in secondary_ticks])\n",
    "\n",
    "    return\n",
    "# end: def set_shared_grid\n",
    "\n",
    "# Customize plot\n",
    "def customize_plot(ax1, ax2, fontsize):\n",
    "    ax1.grid(True, alpha=0.5)\n",
    "    # Customize axis\n",
    "    ax1.set_xlabel('t [h]', fontsize=fontsize)\n",
    "    ax1.set_xlim(left=0)\n",
    "    ax1.set_ylabel(r'$\\varepsilon$ [-]', fontsize=fontsize)\n",
    "    ax1.set_ylim(bottom=0)\n",
    "    ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "    ax2.tick_params(axis='both', labelsize=fontsize)\n",
    "    return ax1, ax2\n",
    "# end: customize_plot\n",
    "\n",
    "# Set same number of thicks on twin axes\n",
    "def match_tick_count(ax_src, ax_tgt, axis='y'):\n",
    "    if axis not in ('x','y'):\n",
    "        raise ValueError(\"`axis` must be 'x' or 'y'\")\n",
    "\n",
    "    # Pick off the existing tick *locations* on the source axis\n",
    "    if axis == 'x':\n",
    "        n = len(ax_src.get_xticks())-1\n",
    "        locator = mticker.MaxNLocator(n)\n",
    "        ax_tgt.xaxis.set_major_locator(locator)\n",
    "    else:\n",
    "        n = len(ax_src.get_yticks())-1\n",
    "        locator = mticker.MaxNLocator(n)\n",
    "        ax_tgt.yaxis.set_major_locator(locator)\n",
    "    return\n",
    "# end: def match_tick_count\n",
    "\n",
    "# Set same tick values on twin axes\n",
    "def match_tick_values(ax_src, ax_tgt, axis='y', copy_labels=True, copy_limits=True):\n",
    "    if axis not in ('x','y'):\n",
    "        raise ValueError(\"`axis` must be 'x' or 'y'\")\n",
    "    # 1) Optionally copy axis limits\n",
    "    if copy_limits:\n",
    "        if axis == 'y':\n",
    "            ax_tgt.set_ylim(ax_src.get_ylim())\n",
    "        else:\n",
    "            ax_tgt.set_xlim(ax_src.get_xlim())\n",
    "\n",
    "    # 2) Grab the *exact* tick positions\n",
    "    if axis == 'y':\n",
    "        ticks = ax_src.get_yticks()\n",
    "        # use a FixedLocator so Matplotlib doesn't try to autoscale them away\n",
    "        ax_tgt.yaxis.set_major_locator(mticker.FixedLocator(ticks))\n",
    "        if copy_labels:\n",
    "            labels = [t.get_text() for t in ax_src.get_yticklabels()]\n",
    "            ax_tgt.set_yticklabels(labels)\n",
    "    else:\n",
    "        ticks = ax_src.get_xticks()\n",
    "        ax_tgt.xaxis.set_major_locator(mticker.FixedLocator(ticks))\n",
    "        if copy_labels:\n",
    "            labels = [t.get_text() for t in ax_src.get_xticklabels()]\n",
    "            ax_tgt.set_xticklabels(labels)\n",
    "    return\n",
    "# end: def match_tick_values\n",
    "\n",
    "# Pick local strain minimum closest to end cycle\n",
    "def snap_minima_to_cycles(min_indices, new_cycle_idx, *, max_gap=None, unique=True):\n",
    "    \n",
    "    min_indices = np.asarray(min_indices)\n",
    "    anchors = np.asarray(new_cycle_idx)\n",
    "\n",
    "    if min_indices.size == 0 or anchors.size == 0:\n",
    "        return np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int)\n",
    "\n",
    "    # Work on a sorted copy of minima for fast nearest-neighbor via searchsorted\n",
    "    mins_sorted = np.sort(np.unique(min_indices))\n",
    "    pos = np.searchsorted(mins_sorted, anchors)\n",
    "\n",
    "    left_idx = np.clip(pos - 1, 0, len(mins_sorted) - 1)\n",
    "    right_idx = np.clip(pos,       0, len(mins_sorted) - 1)\n",
    "\n",
    "    left_vals = mins_sorted[left_idx]\n",
    "    right_vals = mins_sorted[right_idx]\n",
    "\n",
    "    # Choose the closer side\n",
    "    choose_right = np.abs(right_vals - anchors) < np.abs(left_vals - anchors)\n",
    "    nearest = np.where(choose_right, right_vals, left_vals)\n",
    "    dist = np.abs(nearest - anchors)\n",
    "\n",
    "    # Apply max_gap filter if requested\n",
    "    keep = np.ones(len(anchors), dtype=bool)\n",
    "    if max_gap is not None:\n",
    "        keep &= (dist <= max_gap)\n",
    "\n",
    "    nearest = nearest[keep]\n",
    "    anchors_kept = anchors[keep]\n",
    "    dist = dist[keep]\n",
    "\n",
    "    if not unique or nearest.size == 0:\n",
    "        return nearest, anchors_kept, dist\n",
    "\n",
    "    # Resolve duplicates: keep the anchor with smallest distance per chosen minimum\n",
    "    # (ties resolved by first occurrence)\n",
    "    order = np.lexsort((np.arange(len(nearest)), dist))  # sort by distance, then stable index\n",
    "    nearest_sorted = nearest[order]\n",
    "    anchors_sorted = anchors_kept[order]\n",
    "    dist_sorted = dist[order]\n",
    "\n",
    "    # Keep first occurrence of each chosen minimum\n",
    "    uniq_mask = np.ones_like(nearest_sorted, dtype=bool)\n",
    "    uniq_mask[1:] = nearest_sorted[1:] != nearest_sorted[:-1]\n",
    "\n",
    "    picked_minima = nearest_sorted[uniq_mask]\n",
    "    picked_anchors = anchors_sorted[uniq_mask]\n",
    "    picked_dist = dist_sorted[uniq_mask]\n",
    "\n",
    "    # (Optional) restore original anchor order\n",
    "    sort_back = np.argsort(np.argsort(picked_anchors))\n",
    "    return picked_minima[sort_back], picked_anchors[sort_back], picked_dist[sort_back]\n",
    "# end: def snap_minima_to_cycles\n",
    "\n",
    "# Detect unusual spikes in curve\n",
    "def detect_single_point_spikes(y, k=5.0, eps=1e-12):\n",
    "    y = np.asarray(y, float)\n",
    "    dl = np.abs(y[1:-1] - y[:-2])\n",
    "    dr = np.abs(y[1:-1] - y[2:])\n",
    "    diff = np.diff(y)\n",
    "    med = np.median(np.abs(diff - np.median(diff))) + eps\n",
    "    scale = 1.4826 * med\n",
    "    same_side = np.sign(y[1:-1] - y[:-2]) == np.sign(y[1:-1] - y[2:])\n",
    "    spikes_mid = (dl > k*scale) & (dr > k*scale) & same_side\n",
    "    idx = np.where(spikes_mid)[0] + 1\n",
    "    return idx\n",
    "# end: def detect_single_point_spikes\n",
    "\n",
    "# Remove spikes by linear interpolation\n",
    "def fix_spikes_linear(y, idx):\n",
    "    y = np.asarray(y, float).copy()\n",
    "    for i in idx:\n",
    "        if 0 < i < len(y)-1:\n",
    "            y[i] = 0.5*(y[i-1] + y[i+1])\n",
    "    return y\n",
    "# end: def fix_spikes_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare experimental datasets\n",
    "Run the following section to load the raw data from the DVS test and retrieve all datasets corresponding to the available sample types **{RL, RT, TR}**.\n",
    "\n",
    "For each sample type:  \n",
    "- Experiments are grouped by loading degree (LD).  \n",
    "- Raw strain data from DIC analysis are extracted, the pre-mechanosorption part is cut off, and the unloading phases are excluded as they yield unreliable results.\n",
    "- Moisture cycles are reconstructed from the DVS test and aligned with stress and time, while strain data are up-sampled accordingly.  \n",
    "- The elastic strain contribution is added based on the stress and the elastic compliances previously evaluated in [*Ferrara and Wittel (2024)*](https://doi.org/10.1515/hf-2024-0046)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## PREPARE EXPERIMENTAL DATASETS ##################\n",
    "# Load data from dvs test\n",
    "data = np.load(dvs_path)\n",
    "wdata0 = data['w'][0] # moisture of 1st cycle\n",
    "tdata0 = data['t'][0] # time of 1st cycle\n",
    "wdata = data['w'][1] # moisture of following cycles\n",
    "tdata = data['t'][1]  # time of following cycles\n",
    "\n",
    "# Retrieve sample types\n",
    "sample_types = get_sample_types(folder_path)[3:]\n",
    "# Extract datasets\n",
    "dataset = []\n",
    "for sample_type in sample_types:\n",
    "    \n",
    "    # Find experiments of sample type\n",
    "    matches = get_samples_by_type(sample_type, folder_path)\n",
    "    \n",
    "    # Group by loading degree\n",
    "    grouped = defaultdict(list)\n",
    "    for exp, exp_path, sample_name in matches:\n",
    "        ld = np.round(extract_loading_deg(exp_path, sample_name), 1)\n",
    "        grouped[ld].append((exp, exp_path, sample_name))\n",
    "    \n",
    "    # Build dataset entries for each loading degree and sample\n",
    "    for ld, group in grouped.items():\n",
    "        for idx, (exp, exp_path, sample_name) in enumerate(group):\n",
    "\n",
    "            # Prepare file paths and load raw data\n",
    "            file_folder = os.path.join(exp_path, \"Results\", sample_name)\n",
    "            strain_results = load_strain_struct(file_folder)\n",
    "            \n",
    "            total_strain = strain_results['axial_strain']\n",
    "            time = strain_results['exp_duration']\n",
    "            stress = strain_results['stress_meas']\n",
    "            \n",
    "            # Cut off pre‑mechanosorption\n",
    "            cycle_idx = find_cycle_ends(strain_results['RH_mean'], tol=1, merge_gap=5) # find cycle ends\n",
    "            t0 = cycle_idx[0] # get first index = creep pre-mechanosrp.\n",
    "            time = time[t0:] - time[t0]\n",
    "            stress = stress[t0:]\n",
    "            total_strain = total_strain[t0:] - total_strain[t0]\n",
    "            cycle_idx = cycle_idx[1:] - t0\n",
    "            \n",
    "            # Determine load drop\n",
    "            num_cycles = len(cycle_idx)\n",
    "            if num_cycles > 8:\n",
    "                # Drop unloading cycles\n",
    "                guess_drop_idx = (cycle_idx[-2] + cycle_idx[-4]) / 2.\n",
    "                drop_idx = find_local_drop(stress, stress_drop=guess_drop_idx, window=10)\n",
    "                idx_before, drop_idx, idx_after = locate_drop_idx(cycle_idx, drop_idx)\n",
    "                t_full, w_full, s_full = build_dynamic_cycles(tdata0, wdata0, tdata, wdata,\n",
    "                    time, cycle_idx, idx_before, drop_idx, [stress[0], stress[-1]] )\n",
    "            else:\n",
    "                idx_before = drop_idx = idx_after = len(time) - 1\n",
    "                t_full, w_full, s_full = build_dynamic_cycles(tdata0, wdata0, tdata, wdata,\n",
    "                    time, cycle_idx, idx_before, drop_idx, [stress[0]])\n",
    "            \n",
    "            # Find new cycles index\n",
    "            n_pre = np.searchsorted(cycle_idx, idx_before,  side=\"right\")\n",
    "            cycle_len = len(tdata)\n",
    "            new_cycle_idx = [int((i+1)*cycle_len - 1) for i in range(n_pre)]\n",
    "            if (sample_type == 'RT' and ld == 0.5 and idx == 4) or (sample_type == 'RT' and ld == 0.3 and idx in [1,2,3]):\n",
    "                new_cycle_idx = np.concatenate([new_cycle_idx[:-1], [len(t_full)-1]])\n",
    "            else:\n",
    "                new_cycle_idx = np.concatenate([new_cycle_idx, [len(t_full)-1]])\n",
    "            \n",
    "            # Up-sample strain with a spline\n",
    "            spline  = make_interp_spline(time, total_strain, k=5)\n",
    "            eps_full = spline(t_full)\n",
    "            # Find index of local minimum strains\n",
    "            min_indices, _ = find_peaks(-eps_full)\n",
    "            picked_minima, picked_anchors, distances = snap_minima_to_cycles(\n",
    "                min_indices, new_cycle_idx, max_gap=None, unique=True)\n",
    "            picked_minima = np.unique(picked_minima)\n",
    "        \n",
    "            # Cut last cycle out?\n",
    "            t_sel = np.asarray(t_full)[new_cycle_idx]\n",
    "            diffs = np.diff(t_sel)\n",
    "            prev = diffs[:-1]\n",
    "            last = diffs[-1]\n",
    "            baseline = np.median(prev)\n",
    "            tol = 0.5\n",
    "            if abs(last - baseline) > tol:\n",
    "                new_cycle_idx = new_cycle_idx[:-1]\n",
    "\n",
    "            if len(picked_minima) < len(new_cycle_idx):\n",
    "                picked_minima = np.append(picked_minima, new_cycle_idx[-1])\n",
    "            picked_minima = np.sort(picked_minima)\n",
    "            \n",
    "            # Add initial elastic strain\n",
    "            comp_el = fit_elastic_comp(elastic_path)\n",
    "            eps_el = calculate_elastic_strain(comp_el, sample_type, s_full, w_full)\n",
    "            eps_full = eps_full + eps_el[0]\n",
    "\n",
    "            # Append to dataset, including sample_type\n",
    "            dataset.append({\n",
    "                'exp'         : exp,\n",
    "                'sample_type' : sample_type,\n",
    "                'sample_name' : sample_name,\n",
    "                'ld'          : ld,\n",
    "                't_full'      : t_full,\n",
    "                'w_full'      : w_full,\n",
    "                's_full'      : s_full,\n",
    "                'eps_full'    : eps_full,\n",
    "                'w_cycle_idx'   : new_cycle_idx,\n",
    "                'eps_cycle_idx'   : picked_minima,\n",
    "                'stress'      : stress})\n",
    "\n",
    "print(f\"Datasets for samples {format_list(sample_types)} have been retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mechanosorptive strains\n",
    "Run the following section to calculate and plot the mechanosorptive strains. The resulting strains are displayed grouped by sample type and LD, and the corresponding moisture cycles are plotted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CALCULATE AND PLOT MS STRAINS ##################\n",
    "# Prepare figure\n",
    "fontsize = 28\n",
    "ylim1 = 0.04\n",
    "xlim = 21.5\n",
    "linewidth = 2\n",
    "cmap_dict = {0.5: cm.Purples, 0.4: cm.Greens, 0.3: cm.Oranges}\n",
    "_, rep_colors = build_ld_colormaps(dataset, cmap_dict) # create color map\n",
    "fig, axes = plt.subplots(1, len(sample_types), figsize=(7 * len(sample_types), 6.5), constrained_layout=True)\n",
    "\n",
    "# Loop over each sample type\n",
    "results = []\n",
    "for ax1, sample_type in zip(axes, sample_types):\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    # Get datasets for sample type\n",
    "    entries = [e for e in dataset if e['sample_type'] == sample_type]\n",
    "    # Group datasets by LD\n",
    "    colors_by_ld, _ = build_ld_colormaps(entries, cmap_dict)\n",
    "\n",
    "    # Loop through filtered datasets\n",
    "    for entry in entries:\n",
    "        # Extract arrays\n",
    "        w_full = entry['w_full']\n",
    "        t_full = entry['t_full']\n",
    "        s_full = entry['s_full']\n",
    "        eps_full = entry['eps_full']\n",
    "        w_cycle_idx = entry['w_cycle_idx']\n",
    "        eps_cycle_idx = entry['eps_cycle_idx']\n",
    "        ld = entry['ld']\n",
    "        sample_name = entry['sample_name']\n",
    "        stress = entry['stress']\n",
    "        # Set color\n",
    "        color = colors_by_ld[ld].pop(0)      \n",
    "        # Compute mechanosorptive strain\n",
    "        #print(sample_name)\n",
    "        t_flat, w_flat, s_flat, exp_eps_ms, eps_ms_fit, comp_opt = calculate_mechanosorptive_strain(sample_type, w_full, t_full, s_full, eps_full, w_cycle_idx, eps_cycle_idx)\n",
    "\n",
    "        # Plot calculated strains\n",
    "        ax1.plot(t_flat, exp_eps_ms, '-', color=color, linewidth=linewidth)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'sample_name': sample_name,\n",
    "            'sample_type': sample_type,\n",
    "            'ld': ld,\n",
    "            'time': t_flat,\n",
    "            'stress': s_flat,\n",
    "            'w': w_flat,\n",
    "            'eps_ms_fit': eps_ms_fit,\n",
    "            'eps_ms_exp': exp_eps_ms,\n",
    "            'comp_opt': comp_opt,\n",
    "            'compl_ms': eps_ms_fit/s_flat,\n",
    "            'cycle_idx': cycle_idx,\n",
    "            'stress_0': stress})\n",
    "     \n",
    "    # Plot moisture cycles \n",
    "    t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata,  wdata, total_cycles=11, stress_value=s_flat[0], stressed_cycles=9)\n",
    "    ax2.plot(t_new, w_new, linestyle='--', color='grey', label=r'$\\omega$', linewidth = 1.5, alpha=0.5)\n",
    "    # Customize axes\n",
    "    ax1, ax2 = customize_plot(ax1, ax2, fontsize)\n",
    "    ax1.locator_params(axis='x', nbins=5)\n",
    "    ax1.set_xlim(right=xlim)\n",
    "    ax1.set_ylim(top=ylim1)\n",
    "    ax1.locator_params(axis='y', nbins=4)\n",
    "    ax2.set_ylim(bottom = 0.07, top=0.2)\n",
    "    \n",
    "    set_shared_grid(ax1, ax2)\n",
    "    ax2.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "    if sample_type == 'RT' or sample_type == 'TR':\n",
    "        ax1.set_ylabel('')\n",
    "        ax1.set_yticklabels([])\n",
    "    else:\n",
    "        ax1.set_ylabel(r'$\\varepsilon^{ms}$ [-]', fontsize=fontsize)\n",
    "    # Set reference axis for axes customization\n",
    "    if sample_type == 'TR': ax2.set_ylabel(r'$\\omega$ [-]', fontsize=fontsize, rotation=270, labelpad=32)\n",
    "    if sample_type == 'RT' or sample_type == 'RL': ax2.set_yticklabels([])\n",
    "    if sample_type == 'RL': ax_ref = ax1\n",
    "    if sample_type == 'RT': ax_leg = ax1\n",
    "    # Customize title\n",
    "    ax1.set_title(sample_type, fontsize=fontsize)\n",
    "\n",
    "# Customize legend\n",
    "legend_handles = []\n",
    "for ld, color in rep_colors.items():\n",
    "    legend_handles.append(Line2D([0], [0], color=color, lw=linewidth, label=f'{int(ld*100)}% LD'))\n",
    "legend_handles = sorted(legend_handles, key=lambda h: int(h.get_label().replace('% LD', '')))\n",
    "legend_handles.append(Line2D([0], [0], color='black', lw=linewidth, linestyle='--', label='Mean fit'))\n",
    "legend_handles.append(Line2D([0], [0], color='grey', lw=1.5, linestyle='--', alpha=0.5, label=r'$\\omega$'))\n",
    "ax_leg.set_zorder(ax2.get_zorder() + 1)\n",
    "ax_leg.patch.set_alpha(0)\n",
    "leg = ax_leg.legend(handles=legend_handles, fontsize=fontsize-4, loc='upper left')\n",
    "leg.set_zorder=(1000)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot KV-model fitting of mechanosorptive strains\n",
    "Run the following section to calculate and plot the KV-model fitting of the mechanosorptive strains. The resulting strains are displayed grouped by sample type and LD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CALCULATE AND PLOT MS STRAINS FITTING ##################\n",
    "# Prepare figure\n",
    "ylim2 = ylim1\n",
    "fig, axes = plt.subplots(1, len(sample_types), figsize=(6 * len(sample_types), 6), constrained_layout=True)\n",
    "\n",
    "# Loop over each sample type\n",
    "for ax1, sample_type in zip(axes, sample_types):\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    # Get datasets for sample type\n",
    "    entries = [e for e in results if e['sample_type'] == sample_type]\n",
    "    # Group datasets by LD\n",
    "    colors_by_ld, _ = build_ld_colormaps(entries, cmap_dict)\n",
    "\n",
    "    # Group by ld\n",
    "    grouped = defaultdict(list)\n",
    "    for r in entries: grouped[r['ld']].append(r)\n",
    "\n",
    "    # Loop through filtered datasets\n",
    "    for ld_val, group in grouped.items():\n",
    "        eps_fit = []\n",
    "        R2 = []\n",
    "        for rec, color in zip(group, colors_by_ld[ld_val]):\n",
    "            # Extract arrays\n",
    "            t_flat = rec['time']\n",
    "            eps_ms = rec['eps_ms_exp']\n",
    "            w_flat = rec['w']\n",
    "            s_flat = rec['stress']\n",
    "            comp_j = rec['comp_opt']\n",
    "\n",
    "            # Plot with relaxation\n",
    "            t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata,  wdata, total_cycles=11, stress_value=s_flat[0], stressed_cycles=9)\n",
    "            eps_fit.append(mechanosorptive_model(comp_j, s_new, t_new, w_new))\n",
    "            ax1.plot(t_new, eps_fit[-1], '-', color=color, linewidth=linewidth)\n",
    "            \n",
    "            # Calculate coefficient of determination\n",
    "            eps_calc = mechanosorptive_model(comp_j, s_flat, t_flat, w_flat)\n",
    "            R2.append(r2_score(eps_ms, eps_calc))\n",
    "            \n",
    "        # Calculate mean curve (exclude outliers)\n",
    "        exp_eps_ms = trim_mean(eps_fit, 0.3, axis=0) #np.mean(eps_fit, axis=0)\n",
    "        # Fit mean curve\n",
    "        initial_guess = np.array([0.01]*3, dtype=float)\n",
    "        # Residual function for least_squares\n",
    "        def residual(comp_j):\n",
    "            pred = mechanosorptive_model(comp_j, s_new, t_new, w_new)\n",
    "            return (pred - exp_eps_ms)\n",
    "        # Bound compliances to be non‐negative\n",
    "        result = least_squares(residual, initial_guess, bounds=(0, np.inf), xtol=1e-12, ftol=1e-12)\n",
    "        comp_mean = result.x # get prony coefficients\n",
    "        eps_ms_mean = mechanosorptive_model(comp_mean, s_new, t_new, w_new) # mean fitted strain\n",
    "        ax1.plot(t_new, eps_ms_mean, '--', color='black', linewidth=linewidth)\n",
    "\n",
    "        # Calculate mean coefficient of determination\n",
    "        R2_mean = np.mean(R2, axis=0)\n",
    "        comp_str = np.array2string(comp_mean, precision=4, separator=\", \")\n",
    "        print(f\"Sample: {sample_type:>6} | LD: {ld_val:6.2f} | \"\n",
    "            f\"Cⱼ⁻¹: {comp_str} | R²: {R2_mean:.2f}\")\n",
    "        \n",
    "    # Customize axes\n",
    "    ax1, ax2 = customize_plot(ax1, ax2, fontsize)\n",
    "    match_tick_values(ax_ref, ax1, axis='y', copy_labels=True, copy_limits=True)\n",
    "    match_tick_values(ax_ref, ax1, axis='x', copy_labels=True, copy_limits=True)\n",
    "    ax1.set_ylim(top=ylim2)\n",
    "    ax1.set_xlim(right=xlim)\n",
    "    ax2.set_yticks([])\n",
    "    if sample_type == 'RT' or sample_type == 'TR':\n",
    "        ax1.set_ylabel('')\n",
    "        ax1.set_yticklabels([])\n",
    "    else:\n",
    "        ax1.set_ylabel(r'$\\varepsilon^{ms}_{fit}$ [-]', fontsize=fontsize)\n",
    "    if sample_type == 'RT': ax_leg = ax1\n",
    "    # Customize title\n",
    "    ax1.set_title(sample_type, fontsize=fontsize)\n",
    "  \n",
    "# Customize legend\n",
    "legend_handles = []\n",
    "for ld, color in rep_colors.items():\n",
    "    legend_handles.append(Line2D([0], [0], color=color, lw=linewidth, label=f'{int(ld*100)}% LD'))\n",
    "legend_handles = sorted(legend_handles, key=lambda h: int(h.get_label().replace('% LD', '')))\n",
    "legend_handles.append(Line2D([0], [0], color='black', lw=linewidth, linestyle='--', label='Mean fit'))\n",
    "ax_leg.set_zorder(ax2.get_zorder() + 1)\n",
    "ax_leg.patch.set_alpha(0)\n",
    "leg = ax_leg.legend(handles=legend_handles, fontsize=fontsize-4, loc='upper left')\n",
    "leg.set_zorder=(1000)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mean fitted mechanosorptive strain rate\n",
    "\n",
    "Run the following section to plot the evolution of mean fitted mechanosorptive strain rate grouped by sample type and LD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CALCULATE AND PLOT MS STRAIN RATE (TIME) ##################\n",
    "# Prepare figure\n",
    "fig, axes = plt.subplots(1, len(sample_types), figsize=(6 * len(sample_types), 6), constrained_layout=True)\n",
    "\n",
    "# Loop over each sample type\n",
    "for ax1, sample_type in zip(axes, sample_types):\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    # Get datasets for sample type\n",
    "    entries = [e for e in results if e['sample_type'] == sample_type]\n",
    "    # Group datasets by LD\n",
    "    colors_by_ld, _ = build_ld_colormaps(entries, cmap_dict)\n",
    "\n",
    "    # Group by ld\n",
    "    grouped = defaultdict(list)\n",
    "    for r in entries: grouped[r['ld']].append(r)\n",
    "\n",
    "    # Loop through filtered datasets\n",
    "    for ld_val, group in grouped.items():\n",
    "        eps_fit = []\n",
    "        s_store = []\n",
    "        for rec, color in zip(group, colors_by_ld[ld_val]):\n",
    "            # Extract arrays\n",
    "            t_flat = rec['time']\n",
    "            eps_ms_fit = rec['eps_ms_fit']\n",
    "            w_flat = rec['w']\n",
    "            s_flat = rec['stress']\n",
    "            comp_j = rec['comp_opt']\n",
    "\n",
    "            # Fit and extract compliance\n",
    "            t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata,  wdata, total_cycles=11, stress_value=s_flat[0], stressed_cycles=9)\n",
    "            eps_fit.append(mechanosorptive_model(comp_j, s_new, t_new, w_new))\n",
    "            \n",
    "        # Calculate mean curve (exclude outliers)\n",
    "        exp_eps_ms = trim_mean(eps_fit, 0.3, axis=0) #np.mean(eps_fit, axis=0)\n",
    "        # Fit mean curve\n",
    "        initial_guess = np.array([0.01]*3, dtype=float)\n",
    "        # Residual function for least_squares\n",
    "        def residual(comp_j):\n",
    "            pred = mechanosorptive_model(comp_j, s_new, t_new, w_new)\n",
    "            return (pred - exp_eps_ms)\n",
    "        # Bound compliances to be non‐negative\n",
    "        result = least_squares(residual, initial_guess, bounds=(0, np.inf), xtol=1e-12, ftol=1e-12)\n",
    "        comp_mean = result.x # get prony coefficients\n",
    "        eps_ms_mean = mechanosorptive_model(comp_mean, s_new, t_new, w_new) # mean fitted strain\n",
    "        \n",
    "        # Build strain‐rate array (time)\n",
    "        dt = np.diff(t_new)\n",
    "        dw = np.diff(w_new)\n",
    "        deps = np.diff(eps_ms_mean)\n",
    "        eps_t_rate = deps / dt\n",
    "        t_filt = t_new[1:]\n",
    "        # Filter out spikes\n",
    "        idx = detect_single_point_spikes(eps_t_rate, k=5)\n",
    "        eps_t_rate = fix_spikes_linear(eps_t_rate, idx)\n",
    "        ax1.plot(t_filt, eps_t_rate, '-', color=rep_colors[ld_val], linewidth=linewidth)\n",
    "\n",
    "    # Plot moisture\n",
    "    ax2.plot(t_new, w_new, linestyle='--', color='grey', label=r'$\\omega$', linewidth = 1.5, alpha=0.5)\n",
    "    # Customize axes\n",
    "    ax1, ax2 = customize_plot(ax1, ax2, fontsize)\n",
    "    match_tick_values(ax_ref, ax1, axis='x', copy_labels=True, copy_limits=True)\n",
    "    ax1.set_ylim(bottom= -0.006, top=0.012)\n",
    "    match_tick_count(ax_ref, ax1, axis='y')\n",
    "    ax1.set_xlim(right=xlim)\n",
    "    ax2.set_ylim(bottom = 0.07, top=0.2)\n",
    "\n",
    "    set_shared_grid(ax1, ax2)\n",
    "    ax2.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "    if sample_type == 'TR': ax2.set_ylabel(r'$\\omega$ [-]', fontsize=fontsize, rotation=270, labelpad=32)\n",
    "    if sample_type == 'RT' or sample_type == 'RL': ax2.set_yticklabels([])\n",
    "    if sample_type == 'RT' or sample_type == 'TR':\n",
    "        ax1.set_ylabel('')\n",
    "        ax1.set_yticklabels([])\n",
    "    else:\n",
    "        ax1.set_ylabel(r'$\\Delta\\overline{\\varepsilon}^{{ms}}_{{fit}} / \\Delta t$', fontsize=fontsize)\n",
    "    if sample_type == 'RT': ax_leg = ax1\n",
    "    # Customize title\n",
    "    ax1.set_title(sample_type, fontsize=fontsize)\n",
    "\n",
    "# Customize legend\n",
    "legend_handles = []\n",
    "for ld, color in rep_colors.items():\n",
    "    legend_handles.append(Line2D([0], [0], color=color, lw=linewidth, label=f'{int(ld*100)}% LD'))\n",
    "legend_handles = sorted(legend_handles, key=lambda h: int(h.get_label().replace('% LD', '')))\n",
    "legend_handles.append(Line2D([0], [0], color='grey', lw=1.5, linestyle='--', alpha=0.5, label=r'$\\omega$'))\n",
    "ax_leg.set_zorder(ax2.get_zorder() + 1)\n",
    "ax_leg.patch.set_alpha(0)\n",
    "leg = ax_leg.legend(handles=legend_handles, fontsize=fontsize-4, loc='upper right')\n",
    "leg.set_zorder=(1000)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mechanosorptive compliances\n",
    "Run the following section to plot the mechanosorptive compliances calculated from the fitted mechanosorptive strains divided by stress. The resulting compliances are displayed grouped by sample type and LD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CALCULATE AND PLOT MS COMPLIANCES FITTING ##################\n",
    "# Prepare figure\n",
    "ylim3 = 24\n",
    "fig, axes = plt.subplots(1, len(sample_types), figsize=(6 * len(sample_types), 6), constrained_layout=True)\n",
    "\n",
    "# Loop over each sample type\n",
    "for ax1, sample_type in zip(axes, sample_types):\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    # Get datasets for sample type\n",
    "    entries = [e for e in results if e['sample_type'] == sample_type]\n",
    "    # Group datasets by LD\n",
    "    colors_by_ld, _ = build_ld_colormaps(entries, cmap_dict)\n",
    "\n",
    "    # Group by ld\n",
    "    grouped = defaultdict(list)\n",
    "    for r in entries: grouped[r['ld']].append(r)\n",
    "\n",
    "    # Loop through filtered datasets\n",
    "    for ld_val, group in grouped.items():\n",
    "        compl_fit = []\n",
    "        eps_fit = []\n",
    "        s_store = []\n",
    "        for rec, color in zip(group, colors_by_ld[ld_val]):\n",
    "            # Extract arrays\n",
    "            t_flat = rec['time']\n",
    "            eps_ms_fit = rec['eps_ms_fit']\n",
    "            w_flat = rec['w']\n",
    "            s_flat = rec['stress']\n",
    "            comp_j = rec['comp_opt']\n",
    "\n",
    "            # Fit and extract compliance\n",
    "            t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata,  wdata, total_cycles=9, stress_value=s_flat[0], stressed_cycles=9)\n",
    "            eps_fit.append(mechanosorptive_model(comp_j, s_new, t_new, w_new))\n",
    "            compl_fit.append(eps_fit[-1]/s_new) # [1/MPa]\n",
    "            ax1.plot(t_new, compl_fit[-1]*1000, '-', color=color, linewidth=linewidth)\n",
    "            s_store.append(s_new)\n",
    "            \n",
    "        # Calculate mean curve (exclude outliers)\n",
    "        exp_eps_ms = trim_mean(eps_fit, 0.3, axis=0) #np.mean(eps_fit, axis=0)\n",
    "        # Fit mean curve\n",
    "        initial_guess = np.array([0.01]*3, dtype=float)\n",
    "        # Residual function for least_squares\n",
    "        def residual(comp_j):\n",
    "            pred = mechanosorptive_model(comp_j, s_new, t_new, w_new)\n",
    "            return (pred - exp_eps_ms)\n",
    "        # Bound compliances to be non‐negative\n",
    "        result = least_squares(residual, initial_guess, bounds=(0, np.inf), xtol=1e-12, ftol=1e-12)\n",
    "        comp_mean = result.x # get prony coefficients\n",
    "        eps_ms_mean = mechanosorptive_model(comp_mean, s_new, t_new, w_new) # mean fitted strain\n",
    "        s_mean = np.mean(s_store) # mean stress [MPa]\n",
    "        ax1.plot(t_new, eps_ms_mean/s_mean*1000, '--', color='black', linewidth=linewidth)\n",
    "\n",
    "    # Customize axes\n",
    "    ax1, ax2 = customize_plot(ax1, ax2, fontsize)\n",
    "    match_tick_values(ax_ref, ax1, axis='x', copy_labels=True, copy_limits=True)\n",
    "    ax1.set_ylim(top=ylim3)\n",
    "    #ax1.locator_params(axis='y', nbins=5)\n",
    "    match_tick_count(ax_ref, ax1, axis='y')\n",
    "    ax1.set_xlim(right=xlim)\n",
    "    ax2.set_yticks([])\n",
    "    if sample_type == 'RT' or sample_type == 'TR':\n",
    "        ax1.set_ylabel('')\n",
    "        ax1.set_yticklabels([])\n",
    "    else:\n",
    "        ax1.set_ylabel(r'$M_c$ [1/GPa]', fontsize=fontsize)\n",
    "    if sample_type == 'RT': ax_leg = ax1\n",
    "    # Customize title\n",
    "    ax1.set_title(sample_type, fontsize=fontsize)\n",
    "  \n",
    "# Customize legend\n",
    "legend_handles = []\n",
    "for ld, color in rep_colors.items():\n",
    "    legend_handles.append(Line2D([0], [0], color=color, lw=linewidth, label=f'{int(ld*100)}% LD'))\n",
    "legend_handles = sorted(legend_handles, key=lambda h: int(h.get_label().replace('% LD', '')))\n",
    "legend_handles.append(Line2D([0], [0], color='black', lw=linewidth, linestyle='--', label='Mean fit'))\n",
    "ax_leg.set_zorder(ax2.get_zorder() + 1)\n",
    "ax_leg.patch.set_alpha(0)\n",
    "leg = ax_leg.legend(handles=legend_handles, fontsize=fontsize-4, loc='upper left')\n",
    "leg.set_zorder=(1000)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot normalized mechanosorptive compliances\n",
    "Run the following section to plot the normalized mechanosorptive compliances, obtained by dividing the mechanosorptive compliance by the elastic compliance at a given moisture content (set as input in the code). The resulting compliances are displayed grouped by sample type and LD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CALCULATE AND PLOT NORMALIZED MS COMPLIANCES ##################\n",
    "# Extract elastic compliances\n",
    "comp_el = fit_elastic_comp(elastic_path)\n",
    "w_ref = 0.08 # set reference moisture content for normalized compliance\n",
    "\n",
    "# Prepare figure\n",
    "ylim4 = 9\n",
    "fig, axes = plt.subplots(1, len(sample_types), figsize=(6 * len(sample_types), 6), constrained_layout=True)\n",
    "\n",
    "# Loop over each sample type\n",
    "for ax1, sample_type in zip(axes, sample_types):\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    # Get datasets for sample type\n",
    "    entries = [e for e in results if e['sample_type'] == sample_type]\n",
    "    # Group by ld\n",
    "    grouped = defaultdict(list)\n",
    "    for r in entries: grouped[r['ld']].append(r)\n",
    "\n",
    "    # Loop through filtered datasets\n",
    "    for ld_val, group in grouped.items():\n",
    "        compl_fit = []\n",
    "        eps_fit = []\n",
    "        s_store = []\n",
    "        for rec in group:\n",
    "            # Extract arrays\n",
    "            t_flat = rec['time']\n",
    "            eps_ms_fit = rec['eps_ms_fit']\n",
    "            w_flat = rec['w']\n",
    "            s_flat = rec['stress']\n",
    "            comp_j = rec['comp_opt']\n",
    "\n",
    "            # Fit and extract compliance\n",
    "            t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata,  wdata, total_cycles=9, stress_value=s_flat[0], stressed_cycles=9)\n",
    "            eps_fit.append(mechanosorptive_model(comp_j, s_new, t_new, w_new))\n",
    "            compl_fit.append(eps_fit[-1]/s_new*1000) # [GPa]\n",
    "            s_store.append(s_new)\n",
    "            \n",
    "        # Calculate mean curve (exclude outliers)\n",
    "        exp_eps_ms = trim_mean(eps_fit, 0.3, axis=0) #np.mean(eps_fit, axis=0)\n",
    "        # Fit mean curve\n",
    "        initial_guess = np.array([0.01]*3, dtype=float)\n",
    "        # Residual function for least_squares\n",
    "        def residual(comp_j):\n",
    "            pred = mechanosorptive_model(comp_j, s_new, t_new, w_new)\n",
    "            return (pred - exp_eps_ms)\n",
    "        # Bound compliances to be non‐negative\n",
    "        result = least_squares(residual, initial_guess, bounds=(0, np.inf), xtol=1e-12, ftol=1e-12)\n",
    "        comp_mean = result.x # get prony coefficients\n",
    "        eps_ms_mean = mechanosorptive_model(comp_mean, s_new, t_new, w_new) # mean fitted strain\n",
    "        s_mean = np.mean(s_store) # mean stress [MPa]\n",
    "        compl_mean = eps_ms_mean / s_mean\n",
    "\n",
    "        # Calculate elastic compliance\n",
    "        a, b, c = comp_el[sample_type]\n",
    "        C_i = a * w_ref**2 + b * w_ref + c\n",
    "        # Plot normalized compliance\n",
    "        ax1.plot(t_new, compl_mean/C_i, '-', color=rep_colors[ld_val], linewidth=linewidth)\n",
    "    \n",
    "    # Customize axes\n",
    "    ax1, ax2 = customize_plot(ax1, ax2, fontsize)\n",
    "    match_tick_values(ax_ref, ax1, axis='x', copy_labels=True, copy_limits=True)\n",
    "    ax1.set_ylim(top=ylim4)\n",
    "    match_tick_count(ax_ref, ax1, axis='y')\n",
    "    ax1.set_xlim(right=xlim)\n",
    "    ax2.set_yticks([])\n",
    "    if sample_type == 'RT' or sample_type == 'TR':\n",
    "        ax1.set_ylabel('')\n",
    "        ax1.set_yticklabels([])\n",
    "    else:\n",
    "        ax1.set_ylabel(r'$\\overline{M_c}/C_0^{-1}$ [-]', fontsize=fontsize)\n",
    "    if sample_type == 'RT': ax_leg = ax1\n",
    "    # Customize title\n",
    "    ax1.set_title(sample_type, fontsize=fontsize)\n",
    "  \n",
    "# Customize legend\n",
    "legend_handles = []\n",
    "for ld, color in rep_colors.items():\n",
    "    legend_handles.append(Line2D([0], [0], color=color, lw=linewidth, label=f'{int(ld*100)}% LD'))\n",
    "legend_handles = sorted(legend_handles, key=lambda h: int(h.get_label().replace('% LD', '')))\n",
    "ax_leg.set_zorder(ax2.get_zorder() + 1)\n",
    "ax_leg.patch.set_alpha(0)\n",
    "leg = ax_leg.legend(handles=legend_handles, fontsize=fontsize-4, loc='upper left')\n",
    "leg.set_zorder=(1000)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
